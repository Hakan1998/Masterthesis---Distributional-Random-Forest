{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>cu</th>\n",
       "      <th>co</th>\n",
       "      <th>variable</th>\n",
       "      <th>tau</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>training_description</th>\n",
       "      <th>model_type</th>\n",
       "      <th>hyperparameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LS_KDEx_LGBM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.764092</td>\n",
       "      <td>0.651639</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.798535</td>\n",
       "      <td>0.706927</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "      <td>levelset_models</td>\n",
       "      <td>{'param_binSize': 20.0, 'param_weightsByDistance': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LS_KDEx_LGBM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_NO2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.987536</td>\n",
       "      <td>0.961502</td>\n",
       "      <td>0.777616</td>\n",
       "      <td>0.955837</td>\n",
       "      <td>0.871928</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "      <td>levelset_models</td>\n",
       "      <td>{'param_binSize': 20.0, 'param_weightsByDistance': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LS_KDEx_LGBM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_O3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.813612</td>\n",
       "      <td>0.706799</td>\n",
       "      <td>0.683918</td>\n",
       "      <td>1.359494</td>\n",
       "      <td>1.006182</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "      <td>levelset_models</td>\n",
       "      <td>{'param_binSize': 20.0, 'param_weightsByDistance': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LS_KDEx_LGBM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_PM10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.799842</td>\n",
       "      <td>0.893143</td>\n",
       "      <td>0.796496</td>\n",
       "      <td>0.834180</td>\n",
       "      <td>0.901941</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "      <td>levelset_models</td>\n",
       "      <td>{'param_binSize': 20.0, 'param_weightsByDistance': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LS_KDEx_LGBM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_PM2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.840553</td>\n",
       "      <td>0.891214</td>\n",
       "      <td>0.850004</td>\n",
       "      <td>1.040120</td>\n",
       "      <td>0.926806</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "      <td>levelset_models</td>\n",
       "      <td>{'param_binSize': 20.0, 'param_weightsByDistance': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429815</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.153080</td>\n",
       "      <td>-0.172690</td>\n",
       "      <td>-0.160152</td>\n",
       "      <td>-0.187802</td>\n",
       "      <td>-0.160530</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "      <td>basic_models</td>\n",
       "      <td>{'param_min_node_size': 128.0, 'param_num_features': 8.0, 'param_num_trees': 100.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429816</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.137637</td>\n",
       "      <td>-0.161119</td>\n",
       "      <td>-0.154333</td>\n",
       "      <td>-0.175216</td>\n",
       "      <td>-0.151331</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "      <td>basic_models</td>\n",
       "      <td>{'param_min_node_size': 32.0, 'param_num_features': 64.0, 'param_num_trees': 50.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429817</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.139674</td>\n",
       "      <td>-0.157154</td>\n",
       "      <td>-0.148969</td>\n",
       "      <td>-0.172815</td>\n",
       "      <td>-0.153373</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "      <td>basic_models</td>\n",
       "      <td>{'param_min_node_size': 32.0, 'param_num_features': 64.0, 'param_num_trees': 500.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429818</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.132174</td>\n",
       "      <td>-0.158939</td>\n",
       "      <td>-0.150274</td>\n",
       "      <td>-0.166776</td>\n",
       "      <td>-0.147638</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "      <td>basic_models</td>\n",
       "      <td>{'param_min_node_size': 4.0, 'param_num_features': 8.0, 'param_num_trees': 250.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429819</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.138409</td>\n",
       "      <td>-0.160480</td>\n",
       "      <td>-0.151879</td>\n",
       "      <td>-0.168807</td>\n",
       "      <td>-0.148179</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "      <td>basic_models</td>\n",
       "      <td>{'param_min_node_size': 8.0, 'param_num_features': 8.0, 'param_num_trees': 250.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>406710 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          model_name   cu   co              variable  tau  split0_test_score  \\\n",
       "0       LS_KDEx_LGBM  1.0  9.0     Location_1_max_CO  0.1           0.764092   \n",
       "1       LS_KDEx_LGBM  1.0  9.0    Location_1_max_NO2  0.1           0.987536   \n",
       "2       LS_KDEx_LGBM  1.0  9.0     Location_1_max_O3  0.1           0.813612   \n",
       "3       LS_KDEx_LGBM  1.0  9.0   Location_1_max_PM10  0.1           0.799842   \n",
       "4       LS_KDEx_LGBM  1.0  9.0  Location_1_max_PM2.5  0.1           0.840553   \n",
       "...              ...  ...  ...                   ...  ...                ...   \n",
       "429815           DRF  1.0  9.0               dummyID  0.1          -0.153080   \n",
       "429816           DRF  1.0  9.0               dummyID  0.1          -0.137637   \n",
       "429817           DRF  1.0  9.0               dummyID  0.1          -0.139674   \n",
       "429818           DRF  1.0  9.0               dummyID  0.1          -0.132174   \n",
       "429819           DRF  1.0  9.0               dummyID  0.1          -0.138409   \n",
       "\n",
       "        split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0                0.651639           0.609375           0.798535   \n",
       "1                0.961502           0.777616           0.955837   \n",
       "2                0.706799           0.683918           1.359494   \n",
       "3                0.893143           0.796496           0.834180   \n",
       "4                0.891214           0.850004           1.040120   \n",
       "...                   ...                ...                ...   \n",
       "429815          -0.172690          -0.160152          -0.187802   \n",
       "429816          -0.161119          -0.154333          -0.175216   \n",
       "429817          -0.157154          -0.148969          -0.172815   \n",
       "429818          -0.158939          -0.150274          -0.166776   \n",
       "429819          -0.160480          -0.151879          -0.168807   \n",
       "\n",
       "        split4_test_score dataset training_description       model_type  \\\n",
       "0                0.706927     air    ID-Based Training  levelset_models   \n",
       "1                0.871928     air    ID-Based Training  levelset_models   \n",
       "2                1.006182     air    ID-Based Training  levelset_models   \n",
       "3                0.901941     air    ID-Based Training  levelset_models   \n",
       "4                0.926806     air    ID-Based Training  levelset_models   \n",
       "...                   ...     ...                  ...              ...   \n",
       "429815          -0.160530     yaz   Full Data Training     basic_models   \n",
       "429816          -0.151331     yaz   Full Data Training     basic_models   \n",
       "429817          -0.153373     yaz   Full Data Training     basic_models   \n",
       "429818          -0.147638     yaz   Full Data Training     basic_models   \n",
       "429819          -0.148179     yaz   Full Data Training     basic_models   \n",
       "\n",
       "                                                                             hyperparameter  \n",
       "0                                 {'param_binSize': 20.0, 'param_weightsByDistance': False}  \n",
       "1                                 {'param_binSize': 20.0, 'param_weightsByDistance': False}  \n",
       "2                                 {'param_binSize': 20.0, 'param_weightsByDistance': False}  \n",
       "3                                 {'param_binSize': 20.0, 'param_weightsByDistance': False}  \n",
       "4                                 {'param_binSize': 20.0, 'param_weightsByDistance': False}  \n",
       "...                                                                                     ...  \n",
       "429815  {'param_min_node_size': 128.0, 'param_num_features': 8.0, 'param_num_trees': 100.0}  \n",
       "429816   {'param_min_node_size': 32.0, 'param_num_features': 64.0, 'param_num_trees': 50.0}  \n",
       "429817  {'param_min_node_size': 32.0, 'param_num_features': 64.0, 'param_num_trees': 500.0}  \n",
       "429818    {'param_min_node_size': 4.0, 'param_num_features': 8.0, 'param_num_trees': 250.0}  \n",
       "429819    {'param_min_node_size': 8.0, 'param_num_features': 8.0, 'param_num_trees': 250.0}  \n",
       "\n",
       "[406710 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dataset_names_id_based = ['air', 'bakery', 'm5', 'wage', 'yaz']\n",
    "dataset_names_full_data = ['air_random_10', 'bakery_random_50', 'm5_random_30', 'wage', 'yaz']\n",
    "\n",
    "def process_dataset(name, training_type, model_type):\n",
    "    suffix = \"FULLDATASET_\" if training_type == \"Full Data Training\" else \"\"\n",
    "    cv_drf_scores_path = f'/workspaces/Masterthesis-DRF/results/results_by_file/{suffix}cv_drf_scores_{name}.csv'\n",
    "    cv_scores_model_path = f'/workspaces/Masterthesis-DRF/results/results_by_file/{suffix}cv_scores_{model_type}_{name}.csv'\n",
    "\n",
    "    if not os.path.exists(cv_drf_scores_path) or not os.path.exists(cv_scores_model_path):\n",
    "        return None\n",
    "\n",
    "    df_0 = pd.read_csv(cv_drf_scores_path)\n",
    "    df = pd.read_csv(cv_scores_model_path)\n",
    "\n",
    "    if model_type == \"levelset_models\":\n",
    "        required_columns = ['binSize', 'weightsByDistance', 'fold', 'model_name', 'cu', 'co', 'variable', 'dataset_name']\n",
    "        value_columns = ['0.9', '0.75', '0.5', '0.25', '0.1']\n",
    "\n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "\n",
    "        for col in value_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "\n",
    "        reshaped_df = df.melt(\n",
    "            id_vars=required_columns,\n",
    "            value_vars=value_columns,\n",
    "            var_name='tau',\n",
    "            value_name='split_test_score'\n",
    "        )\n",
    "\n",
    "        wide_df = reshaped_df.pivot_table(\n",
    "            index=['binSize', 'weightsByDistance', 'model_name', 'cu', 'co', 'variable', 'tau'],\n",
    "            columns='fold',\n",
    "            values='split_test_score'\n",
    "        ).reset_index()\n",
    "\n",
    "        expected_columns = list(wide_df.columns[:7]) + [f\"split{fold}_test_score\" for fold in wide_df.columns[7:]]\n",
    "        wide_df.columns = expected_columns\n",
    "\n",
    "        combined_df = pd.concat([wide_df, df_0], join='outer', ignore_index=True)\n",
    "    else:\n",
    "        combined_df = pd.concat([df, df_0], join='outer', ignore_index=True)\n",
    "\n",
    "    columns_to_drop = [\n",
    "    'mean_score_time', 'std_score_time',  'std_test_score',\n",
    "    'rank_test_score', 'mean_fit_time', 'std_fit_time', \"mean_test_score\"\n",
    "]\n",
    "\n",
    "    combined_df = combined_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    combined_df['dataset'] = name\n",
    "    combined_df['training_description'] = training_type\n",
    "    combined_df['model_type'] = model_type\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "processed_dfs = []\n",
    "\n",
    "for name in dataset_names_id_based:\n",
    "    for model_type in ['levelset_models', 'basic_models']:\n",
    "        processed_df = process_dataset(name, training_type=\"ID-Based Training\", model_type=model_type)\n",
    "        if processed_df is not None:\n",
    "            processed_dfs.append(processed_df)\n",
    "\n",
    "for name in dataset_names_full_data:\n",
    "    for model_type in ['levelset_models', 'basic_models']:\n",
    "        processed_df = process_dataset(name, training_type=\"Full Data Training\", model_type=model_type)\n",
    "        if processed_df is not None:\n",
    "            processed_dfs.append(processed_df)\n",
    "\n",
    "final_combined_df = pd.concat(processed_dfs, ignore_index=True) if processed_dfs else pd.DataFrame()\n",
    "\n",
    "final_combined_df.drop(columns=['params'], inplace=True)\n",
    "\n",
    "final_combined_df.rename(columns={\n",
    "    'binSize': 'param_binSize',\n",
    "    'weightsByDistance': 'param_weightsByDistance'\n",
    "}, inplace=True)\n",
    "\n",
    "final_combined_df = final_combined_df.drop_duplicates()\n",
    "\n",
    "param_columns = [col for col in final_combined_df.columns if col.startswith('param_')]\n",
    "final_combined_df['hyperparameter'] = final_combined_df[param_columns].apply(\n",
    "    lambda row: {col: row[col] for col in param_columns if pd.notna(row[col])}, axis=1\n",
    ")\n",
    "\n",
    "# Schritt 3: (Optional) Entferne die ursprünglichen 'param_'-Spalten, wenn sie nicht mehr benötigt werden\n",
    "final_combined_df = final_combined_df.drop(columns=param_columns)\n",
    "\n",
    "\n",
    "final_combined_df.to_csv(\"crossValidation_results_allDatasets.csv\", index=False)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(final_combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LS_KDEx_LGBM' 'LS_KDEx_MLP' 'DRF' 'MLP' 'LGBM' 'RFW' 'KNNW' 'DTW' 'GKW']\n"
     ]
    }
   ],
   "source": [
    "print(final_combined_df[\"model_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>cu</th>\n",
       "      <th>co</th>\n",
       "      <th>Model</th>\n",
       "      <th>Pinball Loss</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>delta C</th>\n",
       "      <th>sl</th>\n",
       "      <th>dataset</th>\n",
       "      <th>training_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SAA</td>\n",
       "      <td>0.088849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.195847</td>\n",
       "      <td>OrderedDict([('alpha', 0.0001), ('early_stopping', True), ('layer1', 6), ('layer2', 13), ('learning_rate_init', 0.0005), ('max_iter', 1000), ('solver', 'adam')])</td>\n",
       "      <td>-1.204268</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.178693</td>\n",
       "      <td>OrderedDict([('learning_rate', 0.01), ('max_depth', -1), ('min_data_in_leaf', 20), ('n_estimators', 100), ('num_leaves', 127)])</td>\n",
       "      <td>-1.011205</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RFW</td>\n",
       "      <td>0.109210</td>\n",
       "      <td>OrderedDict([('max_depth', 8), ('max_features', None), ('min_samples_split', 16), ('n_estimators', 50)])</td>\n",
       "      <td>-0.229163</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>KNNW</td>\n",
       "      <td>0.140335</td>\n",
       "      <td>OrderedDict([('n_neighbors', 128)])</td>\n",
       "      <td>-0.579482</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19080</th>\n",
       "      <td>fish</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.032095</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>-0.619814</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19081</th>\n",
       "      <td>koefte</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.020352</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>0.241680</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19082</th>\n",
       "      <td>lamb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.022566</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>0.149625</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19083</th>\n",
       "      <td>shrimp</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.027005</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>-0.280237</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19084</th>\n",
       "      <td>steak</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.017958</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>-0.134825</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17352 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Variable   cu   co        Model  Pinball Loss  \\\n",
       "0      Location_1_max_CO  9.0  1.0          SAA      0.088849   \n",
       "1      Location_1_max_CO  9.0  1.0          MLP      0.195847   \n",
       "2      Location_1_max_CO  9.0  1.0         LGBM      0.178693   \n",
       "3      Location_1_max_CO  9.0  1.0          RFW      0.109210   \n",
       "4      Location_1_max_CO  9.0  1.0         KNNW      0.140335   \n",
       "...                  ...  ...  ...          ...           ...   \n",
       "19080               fish  1.0  9.0  LS_KDEx_MLP      0.032095   \n",
       "19081             koefte  1.0  9.0  LS_KDEx_MLP      0.020352   \n",
       "19082               lamb  1.0  9.0  LS_KDEx_MLP      0.022566   \n",
       "19083             shrimp  1.0  9.0  LS_KDEx_MLP      0.027005   \n",
       "19084              steak  1.0  9.0  LS_KDEx_MLP      0.017958   \n",
       "\n",
       "                                                                                                                                                             Best Params  \\\n",
       "0                                                                                                                                                                    NaN   \n",
       "1      OrderedDict([('alpha', 0.0001), ('early_stopping', True), ('layer1', 6), ('layer2', 13), ('learning_rate_init', 0.0005), ('max_iter', 1000), ('solver', 'adam')])   \n",
       "2                                        OrderedDict([('learning_rate', 0.01), ('max_depth', -1), ('min_data_in_leaf', 20), ('n_estimators', 100), ('num_leaves', 127)])   \n",
       "3                                                               OrderedDict([('max_depth', 8), ('max_features', None), ('min_samples_split', 16), ('n_estimators', 50)])   \n",
       "4                                                                                                                                    OrderedDict([('n_neighbors', 128)])   \n",
       "...                                                                                                                                                                  ...   \n",
       "19080                                                                                                                      {'binSize': 1000, 'weightsByDistance': False}   \n",
       "19081                                                                                                                      {'binSize': 1000, 'weightsByDistance': False}   \n",
       "19082                                                                                                                      {'binSize': 1000, 'weightsByDistance': False}   \n",
       "19083                                                                                                                      {'binSize': 1000, 'weightsByDistance': False}   \n",
       "19084                                                                                                                      {'binSize': 1000, 'weightsByDistance': False}   \n",
       "\n",
       "        delta C   sl dataset training_description  \n",
       "0           NaN  0.9     air    ID-Based Training  \n",
       "1     -1.204268  0.9     air    ID-Based Training  \n",
       "2     -1.011205  0.9     air    ID-Based Training  \n",
       "3     -0.229163  0.9     air    ID-Based Training  \n",
       "4     -0.579482  0.9     air    ID-Based Training  \n",
       "...         ...  ...     ...                  ...  \n",
       "19080 -0.619814  0.1     yaz   Full Data Training  \n",
       "19081  0.241680  0.1     yaz   Full Data Training  \n",
       "19082  0.149625  0.1     yaz   Full Data Training  \n",
       "19083 -0.280237  0.1     yaz   Full Data Training  \n",
       "19084 -0.134825  0.1     yaz   Full Data Training  \n",
       "\n",
       "[17352 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Dataset names for ID-based and full-data training\n",
    "dataset_names_id_based = ['air', 'bakery', 'm5', 'wage', 'yaz']\n",
    "dataset_names_full_data = ['air_random_10', 'bakery_random_50', 'm5_random_30', 'wage', 'yaz']\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "combined_dfs = []\n",
    "\n",
    "# Process ID-based training datasets (Basic Models)\n",
    "for name in dataset_names_id_based:\n",
    "    file_path = f'/workspaces/Masterthesis-DRF/results/results_by_file/results_basic_Models_{name}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['dataset'] = name\n",
    "        df['training_description'] = \"ID-Based Training\"\n",
    "        combined_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for dataset: {name} (Basic Models)\")\n",
    "\n",
    "# Process full-data training datasets (Basic Models)\n",
    "for name in dataset_names_full_data:\n",
    "    file_path = f'/workspaces/Masterthesis-DRF/results/results_by_file/FULLDATASET_results_basic_Models_{name}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['dataset'] = name\n",
    "        df['training_description'] = \"Full Data Training\"\n",
    "        combined_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for FULLDATASET dataset: {name} (Basic Models)\")\n",
    "\n",
    "# Process ID-based training datasets (Levelset Models)\n",
    "for name in dataset_names_id_based:\n",
    "    file_path = f'/workspaces/Masterthesis-DRF/results/results_by_file/results_LevelsetModels_{name}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['dataset'] = name\n",
    "        df['training_description'] = \"ID-Based Training\"\n",
    "        combined_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for dataset: {name} (Levelset Models)\")\n",
    "\n",
    "# Process full-data training datasets (Levelset Models)\n",
    "for name in dataset_names_full_data:\n",
    "    file_path = f'/workspaces/Masterthesis-DRF/results/results_by_file/FULLDATASET_results_LevelsetModels_{name}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['dataset'] = name\n",
    "        df['training_description'] = \"Full Data Training\"\n",
    "        combined_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for FULLDATASET dataset: {name} (Levelset Models)\")\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "final_combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
    "final_combined_df = final_combined_df.drop_duplicates()\n",
    "# Save the final DataFrame to a CSV file\n",
    "final_combined_df.to_csv(\"results_combined_allDatasets.csv\", index=False)\n",
    "\n",
    "# Display the final combined DataFrame\n",
    "display(final_combined_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
