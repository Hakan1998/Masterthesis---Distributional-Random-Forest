{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>cu</th>\n",
       "      <th>co</th>\n",
       "      <th>variable</th>\n",
       "      <th>tau</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>params</th>\n",
       "      <th>dataset</th>\n",
       "      <th>training_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LS_KDEx_LGBM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.764092</td>\n",
       "      <td>0.651639</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.798535</td>\n",
       "      <td>0.706927</td>\n",
       "      <td>{'binSize': 20.0, 'weightsByDistance': False}</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LS_KDEx_LGBM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_NO2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.987536</td>\n",
       "      <td>0.961502</td>\n",
       "      <td>0.777616</td>\n",
       "      <td>0.955837</td>\n",
       "      <td>0.871928</td>\n",
       "      <td>{'binSize': 20.0, 'weightsByDistance': False}</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LS_KDEx_LGBM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_O3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.813612</td>\n",
       "      <td>0.706799</td>\n",
       "      <td>0.683918</td>\n",
       "      <td>1.359494</td>\n",
       "      <td>1.006182</td>\n",
       "      <td>{'binSize': 20.0, 'weightsByDistance': False}</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LS_KDEx_LGBM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_PM10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.799842</td>\n",
       "      <td>0.893143</td>\n",
       "      <td>0.796496</td>\n",
       "      <td>0.834180</td>\n",
       "      <td>0.901941</td>\n",
       "      <td>{'binSize': 20.0, 'weightsByDistance': False}</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LS_KDEx_LGBM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_PM2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.840553</td>\n",
       "      <td>0.891214</td>\n",
       "      <td>0.850004</td>\n",
       "      <td>1.040120</td>\n",
       "      <td>0.926806</td>\n",
       "      <td>{'binSize': 20.0, 'weightsByDistance': False}</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78005</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.153080</td>\n",
       "      <td>-0.172690</td>\n",
       "      <td>-0.160152</td>\n",
       "      <td>-0.187802</td>\n",
       "      <td>-0.160530</td>\n",
       "      <td>{'min_node_size': 128, 'num_features': 8, 'num...</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78006</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.137637</td>\n",
       "      <td>-0.161119</td>\n",
       "      <td>-0.154333</td>\n",
       "      <td>-0.175216</td>\n",
       "      <td>-0.151331</td>\n",
       "      <td>{'min_node_size': 32, 'num_features': 64, 'num...</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78007</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.139674</td>\n",
       "      <td>-0.157154</td>\n",
       "      <td>-0.148969</td>\n",
       "      <td>-0.172815</td>\n",
       "      <td>-0.153373</td>\n",
       "      <td>{'min_node_size': 32, 'num_features': 64, 'num...</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78008</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.132174</td>\n",
       "      <td>-0.158939</td>\n",
       "      <td>-0.150274</td>\n",
       "      <td>-0.166776</td>\n",
       "      <td>-0.147638</td>\n",
       "      <td>{'min_node_size': 4, 'num_features': 8, 'num_t...</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78009</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.138409</td>\n",
       "      <td>-0.160480</td>\n",
       "      <td>-0.151879</td>\n",
       "      <td>-0.168807</td>\n",
       "      <td>-0.148179</td>\n",
       "      <td>{'min_node_size': 8, 'num_features': 8, 'num_t...</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78010 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         model_name   cu   co              variable  tau  split0_test_score  \\\n",
       "0      LS_KDEx_LGBM  1.0  9.0     Location_1_max_CO  0.1           0.764092   \n",
       "1      LS_KDEx_LGBM  1.0  9.0    Location_1_max_NO2  0.1           0.987536   \n",
       "2      LS_KDEx_LGBM  1.0  9.0     Location_1_max_O3  0.1           0.813612   \n",
       "3      LS_KDEx_LGBM  1.0  9.0   Location_1_max_PM10  0.1           0.799842   \n",
       "4      LS_KDEx_LGBM  1.0  9.0  Location_1_max_PM2.5  0.1           0.840553   \n",
       "...             ...  ...  ...                   ...  ...                ...   \n",
       "78005           DRF  1.0  9.0               dummyID  0.1          -0.153080   \n",
       "78006           DRF  1.0  9.0               dummyID  0.1          -0.137637   \n",
       "78007           DRF  1.0  9.0               dummyID  0.1          -0.139674   \n",
       "78008           DRF  1.0  9.0               dummyID  0.1          -0.132174   \n",
       "78009           DRF  1.0  9.0               dummyID  0.1          -0.138409   \n",
       "\n",
       "       split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0               0.651639           0.609375           0.798535   \n",
       "1               0.961502           0.777616           0.955837   \n",
       "2               0.706799           0.683918           1.359494   \n",
       "3               0.893143           0.796496           0.834180   \n",
       "4               0.891214           0.850004           1.040120   \n",
       "...                  ...                ...                ...   \n",
       "78005          -0.172690          -0.160152          -0.187802   \n",
       "78006          -0.161119          -0.154333          -0.175216   \n",
       "78007          -0.157154          -0.148969          -0.172815   \n",
       "78008          -0.158939          -0.150274          -0.166776   \n",
       "78009          -0.160480          -0.151879          -0.168807   \n",
       "\n",
       "       split4_test_score                                             params  \\\n",
       "0               0.706927      {'binSize': 20.0, 'weightsByDistance': False}   \n",
       "1               0.871928      {'binSize': 20.0, 'weightsByDistance': False}   \n",
       "2               1.006182      {'binSize': 20.0, 'weightsByDistance': False}   \n",
       "3               0.901941      {'binSize': 20.0, 'weightsByDistance': False}   \n",
       "4               0.926806      {'binSize': 20.0, 'weightsByDistance': False}   \n",
       "...                  ...                                                ...   \n",
       "78005          -0.160530  {'min_node_size': 128, 'num_features': 8, 'num...   \n",
       "78006          -0.151331  {'min_node_size': 32, 'num_features': 64, 'num...   \n",
       "78007          -0.153373  {'min_node_size': 32, 'num_features': 64, 'num...   \n",
       "78008          -0.147638  {'min_node_size': 4, 'num_features': 8, 'num_t...   \n",
       "78009          -0.148179  {'min_node_size': 8, 'num_features': 8, 'num_t...   \n",
       "\n",
       "      dataset training_description  \n",
       "0         air    ID-Based Training  \n",
       "1         air    ID-Based Training  \n",
       "2         air    ID-Based Training  \n",
       "3         air    ID-Based Training  \n",
       "4         air    ID-Based Training  \n",
       "...       ...                  ...  \n",
       "78005     yaz   Full Data Training  \n",
       "78006     yaz   Full Data Training  \n",
       "78007     yaz   Full Data Training  \n",
       "78008     yaz   Full Data Training  \n",
       "78009     yaz   Full Data Training  \n",
       "\n",
       "[78010 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "dataset_names_id_based = ['air', 'bakery', 'm5', 'wage', 'yaz']\n",
    "dataset_names_full_data = ['air_random_10', 'bakery_random_50', 'm5_random_30', 'wage', 'yaz']\n",
    "\n",
    "def process_dataset(name, training_type):\n",
    "    suffix = \"FULLDATASET_\" if training_type == \"Full Data Training\" else \"\"\n",
    "    cv_drf_scores_path = f'/workspaces/Masterthesis-DRF/results/results_by_file/{suffix}cv_drf_scores_{name}.csv'\n",
    "    cv_scores_levelset_models_path = f'/workspaces/Masterthesis-DRF/results/results_by_file/{suffix}cv_scores_levelset_models_{name}.csv'\n",
    "\n",
    "    if not os.path.exists(cv_drf_scores_path) or not os.path.exists(cv_scores_levelset_models_path):\n",
    "        return None\n",
    "\n",
    "    df_0 = pd.read_csv(cv_drf_scores_path)\n",
    "    df = pd.read_csv(cv_scores_levelset_models_path)\n",
    "\n",
    "    reshaped_df = df.melt(\n",
    "        id_vars=['binSize', 'weightsByDistance', 'fold', 'model_name', 'cu', 'co', 'variable', 'dataset_name'],\n",
    "        value_vars=['0.9', '0.75', '0.5', '0.25', '0.1'],\n",
    "        var_name='tau',\n",
    "        value_name='split_test_score'\n",
    "    )\n",
    "\n",
    "    wide_df = reshaped_df.pivot_table(\n",
    "        index=['binSize', 'weightsByDistance', 'model_name', 'cu', 'co', 'variable', 'tau'],\n",
    "        columns='fold',\n",
    "        values='split_test_score'\n",
    "    ).reset_index()\n",
    "\n",
    "    wide_df.columns = [\n",
    "        'binSize', 'weightsByDistance', 'model_name', 'cu', 'co', 'variable', 'tau', \n",
    "        'split0_test_score', 'split1_test_score', 'split2_test_score', \n",
    "        'split3_test_score', 'split4_test_score'\n",
    "    ]\n",
    "\n",
    "    combined_df = pd.concat([wide_df, df_0], join='outer', ignore_index=True)\n",
    "\n",
    "    columns_to_drop = [\n",
    "        'mean_score_time', 'std_score_time', 'param_min_node_size', \n",
    "        'param_num_features', 'param_num_trees', 'mean_test_score',\n",
    "        'std_test_score', 'rank_test_score', \"mean_fit_time\", \"std_fit_time\"\n",
    "    ]\n",
    "    combined_df = combined_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    combined_df['dataset'] = name\n",
    "    combined_df['training_description'] = training_type\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "processed_dfs = []\n",
    "\n",
    "for name in dataset_names_id_based:\n",
    "    processed_df = process_dataset(name, training_type=\"ID-Based Training\")\n",
    "    if processed_df is not None:\n",
    "        processed_dfs.append(processed_df)\n",
    "\n",
    "for name in dataset_names_full_data:\n",
    "    processed_df = process_dataset(name, training_type=\"Full Data Training\")\n",
    "    if processed_df is not None:\n",
    "        processed_dfs.append(processed_df)\n",
    "\n",
    "final_combined_df = pd.concat(processed_dfs, ignore_index=True) if processed_dfs else pd.DataFrame()\n",
    "\n",
    "def integrate_columns_into_params(df):\n",
    "    df['params'] = df['params'].apply(lambda x: eval(x) if isinstance(x, str) else (x if isinstance(x, dict) else {}))\n",
    "    df['params'] = df.apply(\n",
    "        lambda row: {\n",
    "            **row['params'],\n",
    "            'binSize': row['binSize'] if pd.notnull(row['binSize']) else None,\n",
    "            'weightsByDistance': row['weightsByDistance'] if pd.notnull(row['weightsByDistance']) else None\n",
    "        },\n",
    "        axis=1\n",
    "    )\n",
    "    return df.drop(columns=['binSize', 'weightsByDistance'], errors='ignore')\n",
    "\n",
    "df = integrate_columns_into_params(final_combined_df)\n",
    "df.to_csv(\"crossValidation_results_allDatasets.csv\", index=False)\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>cu</th>\n",
       "      <th>co</th>\n",
       "      <th>Model</th>\n",
       "      <th>Pinball Loss</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>delta C</th>\n",
       "      <th>sl</th>\n",
       "      <th>dataset</th>\n",
       "      <th>training_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SAA</td>\n",
       "      <td>0.088849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.195847</td>\n",
       "      <td>OrderedDict([('alpha', 0.0001), ('early_stoppi...</td>\n",
       "      <td>-1.204268</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.178693</td>\n",
       "      <td>OrderedDict([('learning_rate', 0.01), ('max_de...</td>\n",
       "      <td>-1.011205</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RFW</td>\n",
       "      <td>0.109210</td>\n",
       "      <td>OrderedDict([('max_depth', 8), ('max_features'...</td>\n",
       "      <td>-0.229163</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>KNNW</td>\n",
       "      <td>0.140335</td>\n",
       "      <td>OrderedDict([('n_neighbors', 128)])</td>\n",
       "      <td>-0.579482</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19080</th>\n",
       "      <td>fish</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.032095</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>-0.619814</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19081</th>\n",
       "      <td>koefte</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.020352</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>0.241680</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19082</th>\n",
       "      <td>lamb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.022566</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>0.149625</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19083</th>\n",
       "      <td>shrimp</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.027005</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>-0.280237</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19084</th>\n",
       "      <td>steak</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.017958</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>-0.134825</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19085 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Variable   cu   co        Model  Pinball Loss  \\\n",
       "0      Location_1_max_CO  9.0  1.0          SAA      0.088849   \n",
       "1      Location_1_max_CO  9.0  1.0          MLP      0.195847   \n",
       "2      Location_1_max_CO  9.0  1.0         LGBM      0.178693   \n",
       "3      Location_1_max_CO  9.0  1.0          RFW      0.109210   \n",
       "4      Location_1_max_CO  9.0  1.0         KNNW      0.140335   \n",
       "...                  ...  ...  ...          ...           ...   \n",
       "19080               fish  1.0  9.0  LS_KDEx_MLP      0.032095   \n",
       "19081             koefte  1.0  9.0  LS_KDEx_MLP      0.020352   \n",
       "19082               lamb  1.0  9.0  LS_KDEx_MLP      0.022566   \n",
       "19083             shrimp  1.0  9.0  LS_KDEx_MLP      0.027005   \n",
       "19084              steak  1.0  9.0  LS_KDEx_MLP      0.017958   \n",
       "\n",
       "                                             Best Params   delta C   sl  \\\n",
       "0                                                    NaN       NaN  0.9   \n",
       "1      OrderedDict([('alpha', 0.0001), ('early_stoppi... -1.204268  0.9   \n",
       "2      OrderedDict([('learning_rate', 0.01), ('max_de... -1.011205  0.9   \n",
       "3      OrderedDict([('max_depth', 8), ('max_features'... -0.229163  0.9   \n",
       "4                    OrderedDict([('n_neighbors', 128)]) -0.579482  0.9   \n",
       "...                                                  ...       ...  ...   \n",
       "19080      {'binSize': 1000, 'weightsByDistance': False} -0.619814  0.1   \n",
       "19081      {'binSize': 1000, 'weightsByDistance': False}  0.241680  0.1   \n",
       "19082      {'binSize': 1000, 'weightsByDistance': False}  0.149625  0.1   \n",
       "19083      {'binSize': 1000, 'weightsByDistance': False} -0.280237  0.1   \n",
       "19084      {'binSize': 1000, 'weightsByDistance': False} -0.134825  0.1   \n",
       "\n",
       "      dataset training_description  \n",
       "0         air    ID-Based Training  \n",
       "1         air    ID-Based Training  \n",
       "2         air    ID-Based Training  \n",
       "3         air    ID-Based Training  \n",
       "4         air    ID-Based Training  \n",
       "...       ...                  ...  \n",
       "19080     yaz   Full Data Training  \n",
       "19081     yaz   Full Data Training  \n",
       "19082     yaz   Full Data Training  \n",
       "19083     yaz   Full Data Training  \n",
       "19084     yaz   Full Data Training  \n",
       "\n",
       "[19085 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Dataset names for ID-based and full-data training\n",
    "dataset_names_id_based = ['air', 'bakery', 'm5', 'wage', 'yaz']\n",
    "dataset_names_full_data = ['air_random_10', 'bakery_random_50', 'm5_random_30', 'wage', 'yaz']\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "combined_dfs = []\n",
    "\n",
    "# Process ID-based training datasets (Basic Models)\n",
    "for name in dataset_names_id_based:\n",
    "    file_path = f'/workspaces/Masterthesis-DRF/results/results_by_file/results_basic_Models_{name}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['dataset'] = name\n",
    "        df['training_description'] = \"ID-Based Training\"\n",
    "        combined_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for dataset: {name} (Basic Models)\")\n",
    "\n",
    "# Process full-data training datasets (Basic Models)\n",
    "for name in dataset_names_full_data:\n",
    "    file_path = f'/workspaces/Masterthesis-DRF/results/results_by_file/FULLDATASET_results_basic_Models_{name}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['dataset'] = name\n",
    "        df['training_description'] = \"Full Data Training\"\n",
    "        combined_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for FULLDATASET dataset: {name} (Basic Models)\")\n",
    "\n",
    "# Process ID-based training datasets (Levelset Models)\n",
    "for name in dataset_names_id_based:\n",
    "    file_path = f'/workspaces/Masterthesis-DRF/results/results_by_file/results_LevelsetModels_{name}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['dataset'] = name\n",
    "        df['training_description'] = \"ID-Based Training\"\n",
    "        combined_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for dataset: {name} (Levelset Models)\")\n",
    "\n",
    "# Process full-data training datasets (Levelset Models)\n",
    "for name in dataset_names_full_data:\n",
    "    file_path = f'/workspaces/Masterthesis-DRF/results/results_by_file/FULLDATASET_results_LevelsetModels_{name}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['dataset'] = name\n",
    "        df['training_description'] = \"Full Data Training\"\n",
    "        combined_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for FULLDATASET dataset: {name} (Levelset Models)\")\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "final_combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final_combined_df.to_csv(\"results_combined_allDatasets.csv\", index=False)\n",
    "\n",
    "# Display the final combined DataFrame\n",
    "display(final_combined_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
