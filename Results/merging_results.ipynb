{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116498/1742250390.py:16: DtypeWarning: Columns (5,10,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(cv_scores_model_path)\n",
      "/tmp/ipykernel_116498/1742250390.py:16: DtypeWarning: Columns (5,10,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(cv_scores_model_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>cu</th>\n",
       "      <th>co</th>\n",
       "      <th>variable</th>\n",
       "      <th>tau</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>training_description</th>\n",
       "      <th>model_type</th>\n",
       "      <th>hyperparameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.828810</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.954212</td>\n",
       "      <td>1.023091</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "      <td>levelset_models</td>\n",
       "      <td>{'param_binSize': 20.0, 'param_weightsByDistance': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_NO2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.996892</td>\n",
       "      <td>1.105334</td>\n",
       "      <td>0.990913</td>\n",
       "      <td>0.954829</td>\n",
       "      <td>0.874862</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "      <td>levelset_models</td>\n",
       "      <td>{'param_binSize': 20.0, 'param_weightsByDistance': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_O3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.862751</td>\n",
       "      <td>0.906973</td>\n",
       "      <td>0.884902</td>\n",
       "      <td>1.466059</td>\n",
       "      <td>1.448795</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "      <td>levelset_models</td>\n",
       "      <td>{'param_binSize': 20.0, 'param_weightsByDistance': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_PM10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.979488</td>\n",
       "      <td>0.778530</td>\n",
       "      <td>0.839797</td>\n",
       "      <td>0.960356</td>\n",
       "      <td>0.961558</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "      <td>levelset_models</td>\n",
       "      <td>{'param_binSize': 20.0, 'param_weightsByDistance': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Location_1_max_PM2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.040848</td>\n",
       "      <td>0.884657</td>\n",
       "      <td>0.917911</td>\n",
       "      <td>1.046469</td>\n",
       "      <td>1.191733</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "      <td>levelset_models</td>\n",
       "      <td>{'param_binSize': 20.0, 'param_weightsByDistance': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384585</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.148783</td>\n",
       "      <td>-0.168122</td>\n",
       "      <td>-0.156854</td>\n",
       "      <td>-0.183592</td>\n",
       "      <td>-0.165733</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "      <td>basic_models</td>\n",
       "      <td>{'param_min_node_size': 64.0, 'param_num_features': 8.0, 'param_num_trees': 10.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384586</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.145222</td>\n",
       "      <td>-0.176006</td>\n",
       "      <td>-0.186196</td>\n",
       "      <td>-0.183218</td>\n",
       "      <td>-0.140984</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "      <td>basic_models</td>\n",
       "      <td>{'param_min_node_size': 4.0, 'param_num_features': 64.0, 'param_num_trees': 10.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384587</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.135910</td>\n",
       "      <td>-0.155462</td>\n",
       "      <td>-0.161284</td>\n",
       "      <td>-0.178673</td>\n",
       "      <td>-0.143102</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "      <td>basic_models</td>\n",
       "      <td>{'param_min_node_size': 4.0, 'param_num_features': 64.0, 'param_num_trees': 20.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384588</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.140630</td>\n",
       "      <td>-0.170438</td>\n",
       "      <td>-0.161208</td>\n",
       "      <td>-0.179591</td>\n",
       "      <td>-0.156627</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "      <td>basic_models</td>\n",
       "      <td>{'param_min_node_size': 2.0, 'param_num_features': 8.0, 'param_num_trees': 20.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384589</th>\n",
       "      <td>DRF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dummyID</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.138163</td>\n",
       "      <td>-0.153421</td>\n",
       "      <td>-0.154374</td>\n",
       "      <td>-0.170082</td>\n",
       "      <td>-0.143307</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "      <td>basic_models</td>\n",
       "      <td>{'param_min_node_size': 2.0, 'param_num_features': 64.0, 'param_num_trees': 50.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>361533 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         model_name   cu   co              variable  tau  split0_test_score  \\\n",
       "0       LS_KDEx_MLP  1.0  9.0     Location_1_max_CO  0.1           0.828810   \n",
       "1       LS_KDEx_MLP  1.0  9.0    Location_1_max_NO2  0.1           0.996892   \n",
       "2       LS_KDEx_MLP  1.0  9.0     Location_1_max_O3  0.1           0.862751   \n",
       "3       LS_KDEx_MLP  1.0  9.0   Location_1_max_PM10  0.1           0.979488   \n",
       "4       LS_KDEx_MLP  1.0  9.0  Location_1_max_PM2.5  0.1           1.040848   \n",
       "...             ...  ...  ...                   ...  ...                ...   \n",
       "384585          DRF  1.0  9.0               dummyID  0.1          -0.148783   \n",
       "384586          DRF  1.0  9.0               dummyID  0.1          -0.145222   \n",
       "384587          DRF  1.0  9.0               dummyID  0.1          -0.135910   \n",
       "384588          DRF  1.0  9.0               dummyID  0.1          -0.140630   \n",
       "384589          DRF  1.0  9.0               dummyID  0.1          -0.138163   \n",
       "\n",
       "        split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0                0.737705           0.843750           0.954212   \n",
       "1                1.105334           0.990913           0.954829   \n",
       "2                0.906973           0.884902           1.466059   \n",
       "3                0.778530           0.839797           0.960356   \n",
       "4                0.884657           0.917911           1.046469   \n",
       "...                   ...                ...                ...   \n",
       "384585          -0.168122          -0.156854          -0.183592   \n",
       "384586          -0.176006          -0.186196          -0.183218   \n",
       "384587          -0.155462          -0.161284          -0.178673   \n",
       "384588          -0.170438          -0.161208          -0.179591   \n",
       "384589          -0.153421          -0.154374          -0.170082   \n",
       "\n",
       "        split4_test_score dataset training_description       model_type  \\\n",
       "0                1.023091     air    ID-Based Training  levelset_models   \n",
       "1                0.874862     air    ID-Based Training  levelset_models   \n",
       "2                1.448795     air    ID-Based Training  levelset_models   \n",
       "3                0.961558     air    ID-Based Training  levelset_models   \n",
       "4                1.191733     air    ID-Based Training  levelset_models   \n",
       "...                   ...     ...                  ...              ...   \n",
       "384585          -0.165733     yaz   Full Data Training     basic_models   \n",
       "384586          -0.140984     yaz   Full Data Training     basic_models   \n",
       "384587          -0.143102     yaz   Full Data Training     basic_models   \n",
       "384588          -0.156627     yaz   Full Data Training     basic_models   \n",
       "384589          -0.143307     yaz   Full Data Training     basic_models   \n",
       "\n",
       "                                                                           hyperparameter  \n",
       "0                               {'param_binSize': 20.0, 'param_weightsByDistance': False}  \n",
       "1                               {'param_binSize': 20.0, 'param_weightsByDistance': False}  \n",
       "2                               {'param_binSize': 20.0, 'param_weightsByDistance': False}  \n",
       "3                               {'param_binSize': 20.0, 'param_weightsByDistance': False}  \n",
       "4                               {'param_binSize': 20.0, 'param_weightsByDistance': False}  \n",
       "...                                                                                   ...  \n",
       "384585  {'param_min_node_size': 64.0, 'param_num_features': 8.0, 'param_num_trees': 10.0}  \n",
       "384586  {'param_min_node_size': 4.0, 'param_num_features': 64.0, 'param_num_trees': 10.0}  \n",
       "384587  {'param_min_node_size': 4.0, 'param_num_features': 64.0, 'param_num_trees': 20.0}  \n",
       "384588   {'param_min_node_size': 2.0, 'param_num_features': 8.0, 'param_num_trees': 20.0}  \n",
       "384589  {'param_min_node_size': 2.0, 'param_num_features': 64.0, 'param_num_trees': 50.0}  \n",
       "\n",
       "[361533 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dataset_names_id_based = ['air', 'bakery', 'm5', 'wage', 'yaz']\n",
    "dataset_names_full_data = ['subset_air', 'subset_bakery', 'subset_m5', 'wage', 'yaz']\n",
    "\n",
    "def process_dataset(name, training_type, model_type):\n",
    "    suffix = \"FULLDATASET_\" if training_type == \"Full Data Training\" else \"\"\n",
    "    cv_drf_scores_path = f'/workspaces/Masterthesis-DRF/Results/results_by_file/{suffix}cv_drf_scores_{name}.csv'\n",
    "    cv_scores_model_path = f'/workspaces/Masterthesis-DRF/Results/results_by_file/{suffix}cv_scores_{model_type}_{name}.csv'\n",
    "\n",
    "    if not os.path.exists(cv_drf_scores_path) or not os.path.exists(cv_scores_model_path):\n",
    "        return None\n",
    "\n",
    "    df_0 = pd.read_csv(cv_drf_scores_path)\n",
    "    df = pd.read_csv(cv_scores_model_path)\n",
    "\n",
    "    if model_type == \"levelset_models\":\n",
    "        required_columns = ['binSize', 'weightsByDistance', 'fold', 'model_name', 'cu', 'co', 'variable', 'dataset_name']\n",
    "        value_columns = ['0.9', '0.75', '0.5', '0.25', '0.1']\n",
    "\n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "\n",
    "        for col in value_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "\n",
    "        reshaped_df = df.melt(\n",
    "            id_vars=required_columns,\n",
    "            value_vars=value_columns,\n",
    "            var_name='tau',\n",
    "            value_name='split_test_score'\n",
    "        )\n",
    "\n",
    "        wide_df = reshaped_df.pivot_table(\n",
    "            index=['binSize', 'weightsByDistance', 'model_name', 'cu', 'co', 'variable', 'tau'],\n",
    "            columns='fold',\n",
    "            values='split_test_score'\n",
    "        ).reset_index()\n",
    "\n",
    "        expected_columns = list(wide_df.columns[:7]) + [f\"split{fold}_test_score\" for fold in wide_df.columns[7:]]\n",
    "        wide_df.columns = expected_columns\n",
    "\n",
    "        combined_df = pd.concat([wide_df, df_0], join='outer', ignore_index=True)\n",
    "    else:\n",
    "        combined_df = pd.concat([df, df_0], join='outer', ignore_index=True)\n",
    "\n",
    "    columns_to_drop = [\n",
    "    'mean_score_time', 'std_score_time',  'std_test_score',\n",
    "    'rank_test_score', 'mean_fit_time', 'std_fit_time', \"mean_test_score\"\n",
    "]\n",
    "\n",
    "    combined_df = combined_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    combined_df['dataset'] = name\n",
    "    combined_df['training_description'] = training_type\n",
    "    combined_df['model_type'] = model_type\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "processed_dfs = []\n",
    "\n",
    "for name in dataset_names_id_based:\n",
    "    for model_type in ['levelset_models', 'basic_models']:\n",
    "        processed_df = process_dataset(name, training_type=\"ID-Based Training\", model_type=model_type)\n",
    "        if processed_df is not None:\n",
    "            processed_dfs.append(processed_df)\n",
    "\n",
    "for name in dataset_names_full_data:\n",
    "    for model_type in ['levelset_models', 'basic_models']:\n",
    "        processed_df = process_dataset(name, training_type=\"Full Data Training\", model_type=model_type)\n",
    "        if processed_df is not None:\n",
    "            processed_dfs.append(processed_df)\n",
    "\n",
    "final_combined_df = pd.concat(processed_dfs, ignore_index=True) if processed_dfs else pd.DataFrame()\n",
    "final_combined_df.drop(columns=['params'], inplace=True)\n",
    "\n",
    "final_combined_df.rename(columns={\n",
    "    'binSize': 'param_binSize',\n",
    "    'weightsByDistance': 'param_weightsByDistance'\n",
    "}, inplace=True)\n",
    "\n",
    "final_combined_df = final_combined_df.drop_duplicates()\n",
    "\n",
    "final_combined_df['hyperparameter'] = final_combined_df[param_columns].apply(\n",
    "    lambda row: {col: row[col] for col in param_columns if pd.notna(row[col])}, axis=1\n",
    ")\n",
    "param_columns = [col for col in final_combined_df.columns if col.startswith('param_')]\n",
    "\n",
    "\n",
    "# Schritt 3: (Optional) Entferne die ursprünglichen 'param_'-Spalten, wenn sie nicht mehr benötigt werden\n",
    "final_combined_df = final_combined_df.drop(columns=param_columns)\n",
    "\n",
    "\n",
    "final_combined_df.to_csv(\"crossValidation_results_allDatasets.csv\", index=False)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(final_combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LS_KDEx_MLP' 'DRF' 'MLP' 'RFW' 'KNNW' 'DTW' 'GKW' 'LGBM' 'LS_KDEx_LGBM']\n"
     ]
    }
   ],
   "source": [
    "print(final_combined_df[\"model_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>cu</th>\n",
       "      <th>co</th>\n",
       "      <th>Model</th>\n",
       "      <th>Pinball Loss</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>delta C</th>\n",
       "      <th>sl</th>\n",
       "      <th>dataset</th>\n",
       "      <th>training_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SAA</td>\n",
       "      <td>0.088849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.168138</td>\n",
       "      <td>OrderedDict([('alpha', 0.0001), ('early_stopping', True), ('layer1', 13), ('layer2', 13), ('learning_rate_init', 0.001), ('max_iter', 1000), ('solver', 'adam')])</td>\n",
       "      <td>-0.892402</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RFW</td>\n",
       "      <td>0.106581</td>\n",
       "      <td>OrderedDict([('max_depth', 8), ('max_features', None), ('min_samples_split', 8), ('n_estimators', 250)])</td>\n",
       "      <td>-0.199575</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>KNNW</td>\n",
       "      <td>0.140335</td>\n",
       "      <td>OrderedDict([('n_neighbors', 128)])</td>\n",
       "      <td>-0.579482</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Location_1_max_CO</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DTW</td>\n",
       "      <td>0.138488</td>\n",
       "      <td>OrderedDict([('max_depth', 6), ('min_samples_split', 2)])</td>\n",
       "      <td>-0.558693</td>\n",
       "      <td>0.9</td>\n",
       "      <td>air</td>\n",
       "      <td>ID-Based Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19701</th>\n",
       "      <td>fish</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.032095</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>-0.619814</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19702</th>\n",
       "      <td>koefte</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.020352</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>0.241680</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19703</th>\n",
       "      <td>lamb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.022566</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>0.149625</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19704</th>\n",
       "      <td>shrimp</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.027005</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>-0.280237</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19705</th>\n",
       "      <td>steak</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>LS_KDEx_MLP</td>\n",
       "      <td>0.017958</td>\n",
       "      <td>{'binSize': 1000, 'weightsByDistance': False}</td>\n",
       "      <td>-0.134825</td>\n",
       "      <td>0.1</td>\n",
       "      <td>yaz</td>\n",
       "      <td>Full Data Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16487 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Variable   cu   co        Model  Pinball Loss  \\\n",
       "0      Location_1_max_CO  9.0  1.0          SAA      0.088849   \n",
       "1      Location_1_max_CO  9.0  1.0          MLP      0.168138   \n",
       "2      Location_1_max_CO  9.0  1.0          RFW      0.106581   \n",
       "3      Location_1_max_CO  9.0  1.0         KNNW      0.140335   \n",
       "4      Location_1_max_CO  9.0  1.0          DTW      0.138488   \n",
       "...                  ...  ...  ...          ...           ...   \n",
       "19701               fish  1.0  9.0  LS_KDEx_MLP      0.032095   \n",
       "19702             koefte  1.0  9.0  LS_KDEx_MLP      0.020352   \n",
       "19703               lamb  1.0  9.0  LS_KDEx_MLP      0.022566   \n",
       "19704             shrimp  1.0  9.0  LS_KDEx_MLP      0.027005   \n",
       "19705              steak  1.0  9.0  LS_KDEx_MLP      0.017958   \n",
       "\n",
       "                                                                                                                                                             Best Params  \\\n",
       "0                                                                                                                                                                    NaN   \n",
       "1      OrderedDict([('alpha', 0.0001), ('early_stopping', True), ('layer1', 13), ('layer2', 13), ('learning_rate_init', 0.001), ('max_iter', 1000), ('solver', 'adam')])   \n",
       "2                                                               OrderedDict([('max_depth', 8), ('max_features', None), ('min_samples_split', 8), ('n_estimators', 250)])   \n",
       "3                                                                                                                                    OrderedDict([('n_neighbors', 128)])   \n",
       "4                                                                                                              OrderedDict([('max_depth', 6), ('min_samples_split', 2)])   \n",
       "...                                                                                                                                                                  ...   \n",
       "19701                                                                                                                      {'binSize': 1000, 'weightsByDistance': False}   \n",
       "19702                                                                                                                      {'binSize': 1000, 'weightsByDistance': False}   \n",
       "19703                                                                                                                      {'binSize': 1000, 'weightsByDistance': False}   \n",
       "19704                                                                                                                      {'binSize': 1000, 'weightsByDistance': False}   \n",
       "19705                                                                                                                      {'binSize': 1000, 'weightsByDistance': False}   \n",
       "\n",
       "        delta C   sl dataset training_description  \n",
       "0           NaN  0.9     air    ID-Based Training  \n",
       "1     -0.892402  0.9     air    ID-Based Training  \n",
       "2     -0.199575  0.9     air    ID-Based Training  \n",
       "3     -0.579482  0.9     air    ID-Based Training  \n",
       "4     -0.558693  0.9     air    ID-Based Training  \n",
       "...         ...  ...     ...                  ...  \n",
       "19701 -0.619814  0.1     yaz   Full Data Training  \n",
       "19702  0.241680  0.1     yaz   Full Data Training  \n",
       "19703  0.149625  0.1     yaz   Full Data Training  \n",
       "19704 -0.280237  0.1     yaz   Full Data Training  \n",
       "19705 -0.134825  0.1     yaz   Full Data Training  \n",
       "\n",
       "[16487 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Dataset names for ID-based and full-data training\n",
    "dataset_names_id_based = ['air', 'bakery', 'm5', 'wage', 'yaz']\n",
    "dataset_names_full_data = ['subset_air', 'subset_bakery', 'subset_m5', 'wage', 'yaz']\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "combined_dfs = []\n",
    "\n",
    "# Process ID-based training datasets (Basic Models)\n",
    "for name in dataset_names_id_based:\n",
    "    file_path = f'/workspaces/Masterthesis-DRF/Results/results_by_file/results_basic_Models_{name}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['dataset'] = name\n",
    "        df['training_description'] = \"ID-Based Training\"\n",
    "        combined_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for dataset: {name} (Basic Models)\")\n",
    "\n",
    "# Process full-data training datasets (Basic Models)\n",
    "for name in dataset_names_full_data:\n",
    "    file_path = f'/workspaces/Masterthesis-DRF/Results/results_by_file/FULLDATASET_results_basic_Models_{name}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['dataset'] = name\n",
    "        df['training_description'] = \"Full Data Training\"\n",
    "        combined_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for FULLDATASET dataset: {name} (Basic Models)\")\n",
    "\n",
    "# Process ID-based training datasets (Levelset Models)\n",
    "for name in dataset_names_id_based:\n",
    "    file_path = f'/workspaces/Masterthesis-DRF/Results/results_by_file/results_LevelsetModels_{name}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['dataset'] = name\n",
    "        df['training_description'] = \"ID-Based Training\"\n",
    "        combined_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for dataset: {name} (Levelset Models)\")\n",
    "\n",
    "# Process full-data training datasets (Levelset Models)\n",
    "for name in dataset_names_full_data:\n",
    "    file_path = f'/workspaces/Masterthesis-DRF/Results/results_by_file/FULLDATASET_results_LevelsetModels_{name}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['dataset'] = name\n",
    "        df['training_description'] = \"Full Data Training\"\n",
    "        combined_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for FULLDATASET dataset: {name} (Levelset Models)\")\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "final_combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
    "final_combined_df = final_combined_df.drop_duplicates()\n",
    "# Save the final DataFrame to a CSV file\n",
    "final_combined_df.to_csv(\"results_combined_allDatasets.csv\", index=False)\n",
    "\n",
    "# Display the final combined DataFrame\n",
    "display(final_combined_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
