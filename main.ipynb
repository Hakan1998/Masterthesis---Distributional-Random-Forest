{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single ID Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.shared_imports import *\n",
    "\n",
    "# Verify that the current working directory has changed\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Importiere alle Module\n",
    "from scripts.get_data import get_dataset_settings_singleID, preprocess_data_singleID\n",
    "from Wrapper.wrapper import DRFWrapper, MLPRegressorWrapper\n",
    "from scripts.utils import *\n",
    "from scripts.train_and_evaluate_singleID import (  \n",
    "    append_result, evaluate_and_append_models_singleID, create_cv_folds_singleID,preprocess_per_instance_singleID\n",
    ")\n",
    "from scripts.process_target import process_target_singleID\n",
    "\n",
    "import scripts.config as config\n",
    "from scripts.globals import global_fold_scores, global_cv_results, drf_cv_results\n",
    "\n",
    "# Lade alle Module neu\n",
    "importlib.reload(config)\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  # OpenMP Threads auf 4 beschränken\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'  # Für OpenBLAS\n",
    "os.environ['MKL_NUM_THREADS'] = '1'  # Für Intel MKL (falls verwendet)\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'  # Für NumExpr\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'  # Für MacOS Accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -i https://test.pypi.org/simple/ drf==0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drf import drf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set configurations in config.py file before we start process\n",
    "\n",
    "--> set dataset_name before in config file\n",
    "\n",
    "--> set levelset_calcuations to False if we do the basic models calcuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = config.dataset_name\n",
    "\n",
    "# Hole die Datei-ID für den gewählten Datensatz\n",
    "file_id = {\n",
    "    'bakery': '1r_bDn9Z3Q_XgeTTkJL7352nUG3jkUM0z',\n",
    "    'yaz': '1xrY3Uv5F9F9ofgSM7dVoSK4bE0gPMg36',\n",
    "    'm5': '1tCBaxOgE5HHllvLVeRC18zvALBz6B-6w',\n",
    "    'sid': '1J9bPCfeLDH-mbSnvTHRoCva7pl6cXD3_',\n",
    "    'air': '1SKPpNxulcusNTjRwCC0p3C_XW7aNBNJZ',\n",
    "    \"copula\": '1H5wdJgmxdhbzeS17w0NkRlHRCESEAd-e',\n",
    "    'wage': '1bn7E7NOoRzE4NwXXs1MYhRSKZHC13qYU',\n",
    "}[config.dataset_name]\n",
    "\n",
    "\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "\n",
    "# Datei herunterladen\n",
    "output = f\"{dataset_name}.csv\"\n",
    "gdown.download(url, output, quiet=False)\n",
    "data = pd.read_csv(output)\n",
    "\n",
    "# Erstelle die Dataset-Einstellungen basierend auf den geladenen Daten\n",
    "settings = get_dataset_settings_singleID(data)[dataset_name]\n",
    "\n",
    "y, train_data, test_data, X_train_features, X_test_features, y_train, y_test = preprocess_data_singleID(\n",
    "    data, settings['backscaling_columns'], settings['bool_columns'], settings['drop_columns'])\n",
    "\n",
    "\n",
    "display(X_train_features.head(30))\n",
    "display(y_train.head(3))\n",
    "print(f\"Anzahl der Zeilen: {len(y_train)}\")\n",
    "print(\"Anzahl der targets:\", len(y_train.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process - DDOP Models + Levelset Estimator Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Execution starts here\n",
    "combinations = [(9, 1), (7.5, 2.5), (5, 5), (2.5, 7.5), (1, 9)]\n",
    "table_rows = []\n",
    "random_state = 42\n",
    "\n",
    "# Initialize cvFolds\n",
    "cvFolds = None  # Initialization\n",
    "\n",
    "\n",
    "import scripts.globals as globals  # Import the globals module\n",
    "\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "\n",
    "    # Parallelize column processing within each combination with n_jobs=4 to limit threads\n",
    "    column_results = Parallel(n_jobs=1)(  \n",
    "        delayed(process_target_singleID)(column, cu, co, tau, y_train, X_train_features, X_test_features, y_test, random_state)\n",
    "        for column in y_train.columns\n",
    "    )\n",
    "\n",
    "    # Combine results from all columns and print after each column\n",
    "    for result in column_results:\n",
    "        table_rows.extend(result)\n",
    "        print(table_rows)\n",
    "        # Convert the latest result to a DataFrame and print it\n",
    "        result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "        print(result_table)  # Print the updated results after each column is processed\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "\n",
    "# Define the folder where results will be saved\n",
    "results_folder = \"results\"\n",
    "\n",
    "# Check if the results folder exists, if not, create it\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "# Construct the filename using the format \"results_basic_Models_{dataset_name}.csv\"\n",
    "filename = os.path.join(results_folder, f\"results_basic_Models_{dataset_name}.csv\")\n",
    "\n",
    "# Save the result table to a CSV file in the \"results\" folder\n",
    "result_table.to_csv(filename, index=False)\n",
    "\n",
    "# Aggregate and save cross-validation results at the end of the entire workflow\n",
    "if globals.global_cv_results:\n",
    "    # Concatenate all cross-validation results into a single DataFrame\n",
    "    aggregated_cv_results_df = pd.concat(globals.global_cv_results, ignore_index=True)\n",
    "\n",
    "    # Save the aggregated results to a CSV file in the \"results\" folder\n",
    "    aggregated_cv_filename = os.path.join(results_folder, f\"cv_scores_basic_models_{dataset_name}.csv\")\n",
    "    aggregated_cv_results_df.to_csv(aggregated_cv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter out both 'SAA' and 'LinearRegression' models\n",
    "filtered_table = result_table[(~result_table['Model'].isin(['SAA', 'LR', 'MLP','LGBM'])) & (result_table['sl'] == 0.9)]\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the boxplot\n",
    "sns.boxplot(x='Model', y='delta C', data=filtered_table, showfliers=False, width=0.5, color='lightblue')\n",
    "\n",
    " #Add the stripplot to show the individual data points\n",
    "#sns.stripplot(x='Model', y='delta C', data=filtered_table, color='red', jitter=True, size=6, alpha=0.7)\n",
    "\n",
    "# Add the point plot to show CI based on SD without horizontal lines\n",
    "sns.pointplot(x=\"Model\", y=\"delta C\", data=filtered_table, ci='sd', color='blue', markers=\"o\", scale=0.7, linestyles=\"\")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Boxplot, Stripplot, and Pointplot (CI=SD) of Delta C for Each Model (Excluding SAA and LinearRegression)', fontsize=14)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Delta C', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process - DRF + Levelset Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "\n",
    "timeseries = True\n",
    "\n",
    "# Execution starts here\n",
    "combinations = [(1, 9)]\n",
    "table_rows = []\n",
    "random_state = 1\n",
    "drf_cv_results = []\n",
    "global_fold_scores = []\n",
    "\n",
    "# Iterate over combinations and process them directly\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "    with threadpool_limits(limits=1):\n",
    "            for column in y_train.columns:\n",
    "                print(f\"Processing column: {column}\")\n",
    "\n",
    "                # Preprocess data\n",
    "                X_train_scaled, X_test_scaled, y_train_col, y_test_col, X_train_scaled_withID = preprocess_per_instance_singleID(\n",
    "                    column, X_train_features, X_test_features, y_train, y_test\n",
    "                )\n",
    "                create_cv_folds_singleID(X_train_scaled_withID)\n",
    "            \n",
    "\n",
    "                # SAA model evaluation\n",
    "                saa_model = SampleAverageApproximationNewsvendor(cu, co)\n",
    "                saa_pred = saa_model.fit(y_train_col).predict(X_test_scaled.shape[0])\n",
    "                saa_pinball_loss = pinball_loss(y_test_col.values.flatten(), saa_pred, tau)\n",
    "                append_result(table_rows, column, cu, co, 'SAA', saa_pinball_loss, 'N/A', np.nan, tau)\n",
    "\n",
    "            \n",
    "            \n",
    "                if timeseries:\n",
    "                    # Initialisiere LGBM und MLP-Modelle\n",
    "                    lgbm_model = LGBMRegressor(random_state=random_state, n_jobs=1, verbosity=-1)\n",
    "                    mlp_model = MLPRegressorWrapper(random_state=random_state, early_stopping=True)\n",
    "\n",
    "                    # LGBM-Modell mit GroupSplitting evaluieren\n",
    "                    lgbm_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    lgbm_model_evaluation = [\n",
    "                        ('LS_KDEx_LGBM', LevelSetKDEx(estimator=lgbm_model, binSize=100, weightsByDistance=False), lgbm_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models_singleID(lgbm_model_evaluation, X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_loss, tau, cu, co, column, table_rows, timeseries)\n",
    "\n",
    "                    # MLP-Modell mit GroupSplitting evaluieren\n",
    "                    mlp_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    mlp_model_evaluation = [\n",
    "                        ('LS_KDEx_MLP', LevelSetKDEx(estimator=mlp_model, binSize=100, weightsByDistance=False), mlp_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models_singleID(mlp_model_evaluation, X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_loss, tau, cu, co, column, table_rows, timeseries)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Although the Wage data set is not a timeseries, it works similarly well on group timeseries splits. \n",
    "                    # To do this, we set shuffle = True beforehand in the preprocessing for train/test split. \n",
    "                    # Resulting in an mixed order of the train/test points, even if we order it by the \"Dayindex\" later in the CV splits.\n",
    "                    # saves the extra work and we can work the same split logic for all datasets\n",
    "                    print(\"Only time series Data\")\n",
    "\n",
    "                # DRF-Modell wird immer ausgeführt\n",
    "                drf_model = DRFWrapper(min_node_size=10, num_trees=100, num_threads=12)\n",
    "                drf_grid = get_grid('DRF', X_train_scaled.shape[1])\n",
    "                evaluate_and_append_models_singleID([('DRF', drf_model, drf_grid)], X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_loss, tau, cu, co, column, table_rows, timeseries)\n",
    "\n",
    "\n",
    "                # Print the table after evaluating each column\n",
    "                second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "                print(second_result_table.tail(5))  # Print the last 5 rows of the table after each column is processed\n",
    "\n",
    "\n",
    "# Define the folder where results will be saved\n",
    "results_folder = \"results\"\n",
    "\n",
    "# Check if the results folder exists, if not, create it\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "\n",
    "\n",
    "# Construct the filename and save it in the \"results\" folder\n",
    "filename = os.path.join(results_folder, f\"results_LevelsetModels_{dataset_name}.csv\")\n",
    "second_result_table.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Results saved as {filename}\")\n",
    "\n",
    "# Aggregating fold-wise cross-validation results\n",
    "if global_fold_scores:  # No 'globals' function, just directly use 'global_fold_scores'\n",
    "    # Reset multi-index for all fold scores\n",
    "    global_fold_scores_flat = []\n",
    "    for fold_scores_df in global_fold_scores:\n",
    "        # Reset the multi-index so that the binSize and weightsByDistance become normal columns\n",
    "        flat_df = fold_scores_df.reset_index()\n",
    "        global_fold_scores_flat.append(flat_df)\n",
    "\n",
    "    # Concatenate all fold-wise cross-validation results into a single DataFrame\n",
    "    aggregated_fold_scores_df = pd.concat(global_fold_scores_flat, ignore_index=True)\n",
    "\n",
    "    # Save the aggregated results to a CSV file in the \"results\" folder\n",
    "    aggregated_fold_scores_filename = os.path.join(results_folder, f\"cv_scores_levelset_models_{dataset_name}.csv\")\n",
    "    aggregated_fold_scores_df.to_csv(aggregated_fold_scores_filename, index=False)\n",
    "\n",
    "    ### DRF DATA INSERTED INTO THE MAIN TABLE WHERE OTHER BAYES CVs ARE STORED\n",
    "    aggregated_drf_cv_results_df = pd.concat(drf_cv_results, ignore_index=True)\n",
    "\n",
    "    # Save the aggregated DRF results to a CSV file in the \"results\" folder\n",
    "    aggregated_cv_filename = os.path.join(results_folder, f\"cv_drf_scores_{dataset_name}.csv\")\n",
    "    aggregated_drf_cv_results_df.to_csv(aggregated_cv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter out specific models and only include rows where 'sl' equals 0.9\n",
    "filtered_table = second_result_table[\n",
    "    (~second_result_table['Model'].isin(['SAA', 'LR', 'MLP', 'LGBM'])) & \n",
    "    (second_result_table['sl'] == 0.9)\n",
    "]\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the boxplot\n",
    "sns.boxplot(x='Model', y='delta C', data=filtered_table, showfliers=False, width=0.5, color='lightblue')\n",
    "\n",
    "# Add the point plot to show CI based on SD without horizontal lines\n",
    "sns.pointplot(x=\"Model\", y=\"delta C\", data=filtered_table, ci='sd', color='blue', markers=\"o\", scale=0.7, linestyles=\"\")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Boxplot and Pointplot (CI=SD) of Delta C for Each Model (Excluding SAA and LinearRegression)', fontsize=14)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Delta C', fontsize=12)\n",
    "\n",
    "# Show the plot with tight layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FULL Dataset Cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.shared_imports import *\n",
    "\n",
    "# Verify that the current working directory has changed\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Importiere alle Module\n",
    "from scripts.get_data import get_dataset_settings_alldata, preprocess_data_alldata\n",
    "from Wrapper.wrapper import DRFWrapper, MLPRegressorWrapper\n",
    "from scripts.utils import *\n",
    "from scripts.train_and_evaluate_alldata import (  \n",
    "    evaluate_and_append_models_alldata, create_cv_folds_alldata, preprocess_per_instance_alldata\n",
    ")\n",
    "from scripts.process_target import process_target_alldata\n",
    "\n",
    "import scripts.config as config\n",
    "from scripts.globals import global_fold_scores, global_cv_results, drf_cv_results\n",
    "\n",
    "# Lade alle Module neu\n",
    "importlib.reload(config)\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  # OpenMP Threads auf 4 beschränken\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'  # Für OpenBLAS\n",
    "os.environ['MKL_NUM_THREADS'] = '1'  # Für Intel MKL (falls verwendet)\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'  # Für NumExpr\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'  # Für MacOS Accelerate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = config.dataset_name\n",
    "\n",
    "# Hole die Datei-ID für den gewählten Datensatz\n",
    "file_id = {\n",
    "    'bakery': '1r_bDn9Z3Q_XgeTTkJL7352nUG3jkUM0z',\n",
    "    'yaz': '1xrY3Uv5F9F9ofgSM7dVoSK4bE0gPMg36',\n",
    "    'm5': '1tCBaxOgE5HHllvLVeRC18zvALBz6B-6w',\n",
    "    'sid': '1J9bPCfeLDH-mbSnvTHRoCva7pl6cXD3_',\n",
    "    'air': '1DMOaV92n3BFEGeCubaxEys2eLzg2Cic3',\n",
    "    \"copula\": '1H5wdJgmxdhbzeS17w0NkRlHRCESEAd-e',\n",
    "    'wage': '1bn7E7NOoRzE4NwXXs1MYhRSKZHC13qYU',\n",
    "}[config.dataset_name]\n",
    "\n",
    "\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "\n",
    "# Datei herunterladen\n",
    "output = f\"{dataset_name}.csv\"\n",
    "gdown.download(url, output, quiet=False)\n",
    "data = pd.read_csv(output)\n",
    "\n",
    "# Erstelle die Dataset-Einstellungen basierend auf den geladenen Daten\n",
    "settings = get_dataset_settings_alldata(data)[dataset_name]\n",
    "sample_size = 50\n",
    "\n",
    "y, train_data, test_data, X_train_features, X_test_features, y_train, y_test, data, dataset_name  = preprocess_data_alldata(\n",
    "    data=data,\n",
    "    dataset_name=dataset_name,\n",
    "    bool_columns=settings['bool_columns'],\n",
    "    drop_columns=settings['drop_columns'],\n",
    "    sample_size=sample_size\n",
    ")\n",
    "\n",
    "display(y_train.head(3))\n",
    "print(f\"Anzahl der Zeilen: {len(y_train)}\")\n",
    "print(\"Anzahl der targets:\", len(y_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDOP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Execution starts here\n",
    "combinations = [(9, 1), (7.5, 2.5), (5, 5), (2.5, 7.5), (1, 9)]\n",
    "table_rows = []\n",
    "random_state = 1\n",
    "global_cv_results = []\n",
    "\n",
    "# Initialize cvFolds\n",
    "cvFolds = None  # Initialization\n",
    "\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "\n",
    "    # Parallelize column processing within each combination with n_jobs=4 to limit threads\n",
    "    column_results = Parallel(n_jobs=1)(\n",
    "        delayed(process_target_alldata)(column, cu, co, tau, y_train, X_train_features, X_test_features, y_test, random_state)\n",
    "        for column in y_train.columns\n",
    "    )\n",
    "\n",
    "    # Combine results from all columns and print after each column\n",
    "    for result in column_results:\n",
    "        table_rows.extend(result)\n",
    "        print(table_rows)\n",
    "        # Convert the latest result to a DataFrame and print it\n",
    "        result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "        print(result_table)  # Print the updated results after each column is processed\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "\n",
    "# Construct the filename using the format \"results_basicModels_{dataset_name}.csv\"\n",
    "filename = f\"FULLDATASET_results_basic_Models_{dataset_name}.csv\"\n",
    "result_table.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Results saved as {filename}\")\n",
    "\n",
    "# Aggregate and save cross-validation results at the end of the entire workflow\n",
    "if global_cv_results:\n",
    "    # Concatenate all cross-validation results into a single DataFrame\n",
    "    aggregated_cv_results_df = pd.concat(global_cv_results, ignore_index=True)\n",
    "\n",
    "    # Print a summary of the aggregated cross-validation data to verify it looks correct\n",
    "    print(\"Aggregated cross-validation results sample:\")\n",
    "    print(aggregated_cv_results_df.head(5))  # Print the first 5 rows as a sample\n",
    "\n",
    "    # Save the aggregated results to a CSV file\n",
    "    aggregated_cv_filename = f\"FULLDATASET_cv_scores_basic_models_{dataset_name}.csv\"\n",
    "    aggregated_cv_results_df.to_csv(aggregated_cv_filename, index=False)\n",
    "    print(f\"Aggregated cross-validation results saved as {aggregated_cv_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRF+Leveset Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = True\n",
    "import scripts.config as config\n",
    "from scripts.config import *\n",
    "importlib.reload(config)\n",
    "\n",
    "print(n_jobs)\n",
    "\n",
    "# Execution starts here\n",
    "combinations = [(9, 1), (7.5, 2.5), (5, 5), (2.5, 7.5), (1, 9)]\n",
    "\n",
    "table_rows = []\n",
    "random_state = 1\n",
    "drf_cv_results = []\n",
    "global_fold_scores = []\n",
    "\n",
    "# Iterate over combinations and process them directly\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "    with threadpool_limits(limits=1):\n",
    "            for column in y_train.columns:\n",
    "                print(f\"Processing column: {column}\")\n",
    "\n",
    "                              # Preprocess data\n",
    "                X_train_scaled, X_test_scaled, y_train_col, y_test_col, X_train_scaled_withID, X_test_scaled_withID = preprocess_per_instance_alldata(\n",
    "                    column, X_train_features, X_test_features, y_train, y_test\n",
    "                )\n",
    "\n",
    "                create_cv_folds_alldata(X_train_scaled_withID)\n",
    "                \n",
    "                # SAA model\n",
    "                saa_model = SampleAverageApproximationNewsvendor(cu, co)\n",
    "                saa_pred = saa_model.fit(y_train_col).predict(X_test_scaled.shape[0])\n",
    "\n",
    "\n",
    "                # Ensure id_for_CV, y_true, and y_pred are 1-D arrays\n",
    "                id_for_CV = X_test_scaled_withID['id_for_CV'].values.flatten()\n",
    "                y_true = y_test_col.values.flatten()\n",
    "                y_pred = saa_pred.flatten()  # Flatten y_pred to ensure it's 1-D\n",
    "\n",
    "                # Create DataFrame for SAA predictions\n",
    "                saa_predictions_df = pd.DataFrame({\n",
    "                    'id_for_CV': id_for_CV,\n",
    "                    'y_true': y_true,\n",
    "                    'y_pred': y_pred  # Use the flattened y_pred here\n",
    "                })\n",
    "\n",
    "                saa_pinball_losses_per_id = {}\n",
    "                grouped_saa = saa_predictions_df.groupby('id_for_CV')\n",
    "                for id_val, group in grouped_saa:\n",
    "                    y_true_id = group['y_true'].values\n",
    "                    y_pred_id = group['y_pred'].values\n",
    "                    pinball_loss_id = pinball_loss(y_true_id, y_pred_id, tau)\n",
    "                    saa_pinball_losses_per_id[id_val] = pinball_loss_id\n",
    "                    append_result(table_rows, id_val, cu, co, 'SAA', pinball_loss_id, 'N/A', np.nan, tau)\n",
    "\n",
    "                n_features = X_train_scaled.shape[1]\n",
    "\n",
    "                drf_model = DRFWrapper(min_node_size=10, num_trees=100, num_threads=2)\n",
    "                drf_grid = get_grid('DRF', X_train_scaled.shape[1])\n",
    "                \n",
    "                print(f\"Length of X_train_scaled: {len(X_train_scaled)}\")\n",
    "                print(f\"Length of X_test_scaled: {len(X_test_scaled)}\")\n",
    "                evaluate_and_append_models_alldata([('DRF', drf_model, drf_grid)], X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_losses_per_id, tau, cu, co, column, table_rows, timeseries, X_test_scaled_withID)\n",
    "\n",
    "\n",
    "            \n",
    "                if timeseries:\n",
    "                    # Initialisiere LGBM und MLP-Modelle\n",
    "                    lgbm_model = LGBMRegressor(random_state=random_state, n_jobs=n_jobs, verbosity=-1)\n",
    "                    mlp_model = MLPRegressorWrapper(random_state=random_state, early_stopping=True)\n",
    "\n",
    "                    # LGBM-Modell mit GroupSplitting evaluieren\n",
    "                    lgbm_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    print(lgbm_model_params)\n",
    "\n",
    "                    lgbm_model_evaluation = [\n",
    "                        ('LS_KDEx_LGBM', LevelSetKDEx(estimator=lgbm_model, binSize=100, weightsByDistance=False), lgbm_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models_alldata(lgbm_model_evaluation, X_train_scaled, X_test_scaled,\n",
    "                                       y_train_col, y_test_col, saa_pinball_losses_per_id,\n",
    "                                       tau, cu, co, column, table_rows, timeseries, X_test_scaled_withID)\n",
    "\n",
    "                    # MLP-Modell mit GroupSplitting evaluieren\n",
    "                    mlp_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    mlp_model_evaluation = [\n",
    "                        ('LS_KDEx_MLP', LevelSetKDEx(estimator=mlp_model, binSize=100, weightsByDistance=False), mlp_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models_alldata(mlp_model_evaluation, X_train_scaled, X_test_scaled,\n",
    "                                       y_train_col, y_test_col, saa_pinball_losses_per_id,\n",
    "                                       tau, cu, co, column, table_rows, timeseries, X_test_scaled_withID)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Although the Wage data set is not a timeseries, it works similarly well on group timeseries splits. \n",
    "                    # To do this, we set shuffle = True once in the train/test split. \n",
    "                    # Resulting in an mixed order of the train/test points, even if we order it by the Index.\n",
    "                    print(\"Only time series Data\")\n",
    "\n",
    "                # Print the table after evaluating each column\n",
    "                second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "                print(second_result_table.tail(5))  # Print the last 5 rows of the table after each column is processed\n",
    "\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "filename = f\"FULLDATASET_results_LevelsetModels_{dataset_name}.csv\"\n",
    "\n",
    "\n",
    "second_result_table.to_csv(filename, index=False)\n",
    "# Define the folder where results will be saved\n",
    "results_folder = \"results\"\n",
    "\n",
    "from scripts.globals import global_fold_scores, global_cv_results, drf_cv_results\n",
    "\n",
    "\n",
    "# Construct the filename and save it in the \"results\" folder\n",
    "filename = os.path.join(results_folder, f\"FULLDATASET_results_LevelsetModels_{dataset_name}.csv\")\n",
    "second_result_table.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Results saved as {filename}\")\n",
    "\n",
    "# Aggregating fold-wise cross-validation results\n",
    "if global_fold_scores:  # No 'globals' function, just directly use 'global_fold_scores'\n",
    "    # Reset multi-index for all fold scores\n",
    "    global_fold_scores_flat = []\n",
    "    for fold_scores_df in global_fold_scores:\n",
    "        # Reset the multi-index so that the binSize and weightsByDistance become normal columns\n",
    "        flat_df = fold_scores_df.reset_index()\n",
    "        global_fold_scores_flat.append(flat_df)\n",
    "\n",
    "    # Concatenate all fold-wise cross-validation results into a single DataFrame\n",
    "    aggregated_fold_scores_df = pd.concat(global_fold_scores_flat, ignore_index=True)\n",
    "\n",
    "    # Save the aggregated results to a CSV file in the \"results\" folder\n",
    "    aggregated_fold_scores_filename = os.path.join(results_folder, f\"FULLDATASET_cv_scores_levelset_models_{dataset_name}.csv\")\n",
    "    aggregated_fold_scores_df.to_csv(aggregated_fold_scores_filename, index=False)\n",
    "\n",
    "    ### DRF DATA INSERTED INTO THE MAIN TABLE WHERE OTHER BAYES CVs ARE STORED\n",
    "    aggregated_drf_cv_results_df = pd.concat(drf_cv_results, ignore_index=True)\n",
    "\n",
    "    # Save the aggregated DRF results to a CSV file in the \"results\" folder\n",
    "    aggregated_cv_filename = os.path.join(results_folder, f\"cv_drf_scores_{dataset_name}.csv\")\n",
    "    aggregated_drf_cv_results_df.to_csv(aggregated_cv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter die Tabelle, um nur die Zeilen mit dem Modell \"SAA\" anzuzeigen\n",
    "saa_results = result_table[result_table['Model'] == 'SAA']\n",
    "\n",
    "# Zeige die ersten 20 Zeilen der gefilterten Tabelle an\n",
    "print(saa_results.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
