{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single ID Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.shared_imports import *\n",
    "\n",
    "# Verify that the current working directory has changed\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Importiere alle Module\n",
    "from scripts.get_data import get_dataset_settings_singleID, preprocess_data_singleID\n",
    "from Wrapper.wrapper import DRFWrapper, MLPRegressorWrapper\n",
    "from scripts.utils import *\n",
    "from scripts.train_and_evaluate_singleID import (  \n",
    "    append_result, evaluate_and_append_models_singleID, create_cv_folds_singleID,preprocess_per_instance_singleID\n",
    ")\n",
    "from scripts.process_target import process_target_singleID\n",
    "\n",
    "import scripts.config as config\n",
    "from scripts.globals import global_fold_scores, global_cv_results, drf_cv_results\n",
    "\n",
    "# Lade alle Module neu\n",
    "importlib.reload(config)\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  \n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'  \n",
    "os.environ['MKL_NUM_THREADS'] = '1'  \n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'  \n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(config)\n",
    "dataset_name = config.dataset_name\n",
    "\n",
    "# Hole die Datei-ID für den gewählten Datensatz\n",
    "file_id = {\n",
    "    'bakery': '1r_bDn9Z3Q_XgeTTkJL7352nUG3jkUM0z',\n",
    "    'yaz': '1xrY3Uv5F9F9ofgSM7dVoSK4bE0gPMg36',\n",
    "    'm5': '1tCBaxOgE5HHllvLVeRC18zvALBz6B-6w',\n",
    "    'air': '1SKPpNxulcusNTjRwCC0p3C_XW7aNBNJZ',\n",
    "    'wage': '1bn7E7NOoRzE4NwXXs1MYhRSKZHC13qYU',\n",
    "}[config.dataset_name]\n",
    "\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "output = f\"{dataset_name}.csv\"\n",
    "gdown.download(url, output, quiet=False)\n",
    "data = pd.read_csv(output)\n",
    "\n",
    "settings = get_dataset_settings_singleID(data)[dataset_name]\n",
    "\n",
    "y, train_data, test_data, X_train_features, X_test_features, y_train, y_test = preprocess_data_singleID(\n",
    "    data, settings['backscaling_columns'], settings['bool_columns'], settings['drop_columns'])\n",
    "\n",
    "# note: \n",
    "# This is not yet the final data. Data is processed further in the preprocess_per_instance function before the models are trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process - DDOP Models + Levelset Estimator Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the results for the ddop models and the Levelset estimator models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "\n",
    "combinations = [(9, 1), (7.5, 2.5), (5, 5), (2.5, 7.5), (1, 9)]\n",
    "table_rows = []\n",
    "random_state = 42\n",
    "cvFolds = None \n",
    "\n",
    "import scripts.globals as globals  # Import the globals module\n",
    "\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "\n",
    "    # Parallelize column processing within each combination\n",
    "    column_results = Parallel(n_jobs=1)(  \n",
    "        delayed(process_target_singleID)(column, cu, co, tau, y_train, X_train_features, X_test_features, y_test, random_state)\n",
    "        for column in y_train.columns\n",
    "    )\n",
    "\n",
    "    # Combine results from all columns and print after each column\n",
    "    for result in column_results:\n",
    "        table_rows.extend(result)\n",
    "        print(table_rows)\n",
    "        result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "        print(result_table) \n",
    "\n",
    "result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "\n",
    "# Define the folder where results will be saved\n",
    "results_folder = \"results\"\n",
    "\n",
    "# Check if the results folder exists, if not, create it\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "\n",
    "# save Data\n",
    "filename = os.path.join(results_folder, f\"results_basic_Models_{dataset_name}.csv\")\n",
    "result_table.to_csv(filename, index=False)\n",
    "\n",
    "# Aggregate and save cross-validation results at the end of the entire workflow\n",
    "if globals.global_cv_results:\n",
    "    aggregated_cv_results_df = pd.concat(globals.global_cv_results, ignore_index=True)\n",
    "    aggregated_cv_filename = os.path.join(results_folder, f\"cv_scores_basic_models_{dataset_name}.csv\")\n",
    "    aggregated_cv_results_df.to_csv(aggregated_cv_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process - DRF + Levelset Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculates the results for the DRF and takes the tuned hyperparamter of the Levelset estimators to create the LSX results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "\n",
    "timeseries = True\n",
    "\n",
    "# Execution starts here\n",
    "combinations = [(1, 9)]\n",
    "table_rows = []\n",
    "random_state = 1\n",
    "drf_cv_results = []\n",
    "global_fold_scores = []\n",
    "\n",
    "# Iterate over combinations and process them directly\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "    with threadpool_limits(limits=1):\n",
    "            for column in y_train.columns:\n",
    "                print(f\"Processing column: {column}\")\n",
    "\n",
    "                # Preprocess data\n",
    "                X_train_scaled, X_test_scaled, y_train_col, y_test_col, X_train_scaled_withID = preprocess_per_instance_singleID(\n",
    "                    column, X_train_features, X_test_features, y_train, y_test\n",
    "                )\n",
    "                create_cv_folds_singleID(X_train_scaled_withID)\n",
    "\n",
    "                # SAA model evaluation\n",
    "                saa_model = SampleAverageApproximationNewsvendor(cu, co)\n",
    "                saa_pred = saa_model.fit(y_train_col).predict(X_test_scaled.shape[0])\n",
    "                saa_pinball_loss = pinball_loss(y_test_col.values.flatten(), saa_pred, tau)\n",
    "                append_result(table_rows, column, cu, co, 'SAA', saa_pinball_loss, 'N/A', np.nan, tau)\n",
    "\n",
    "            \n",
    "            \n",
    "                if timeseries:\n",
    "                    # Initialisiere LGBM und MLP-Modelle\n",
    "                    lgbm_model = LGBMRegressor(random_state=random_state, n_jobs=1, verbosity=-1)\n",
    "                    mlp_model = MLPRegressorWrapper(random_state=random_state, early_stopping=True)\n",
    "\n",
    "                    lgbm_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    lgbm_model_evaluation = [\n",
    "                        ('LS_KDEx_LGBM', LevelSetKDEx(estimator=lgbm_model, binSize=100, weightsByDistance=False), lgbm_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models_singleID(lgbm_model_evaluation, X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_loss, tau, cu, co, column, table_rows, timeseries)\n",
    "\n",
    "                    mlp_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    mlp_model_evaluation = [\n",
    "                        ('LS_KDEx_MLP', LevelSetKDEx(estimator=mlp_model, binSize=100, weightsByDistance=False), mlp_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models_singleID(mlp_model_evaluation, X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_loss, tau, cu, co, column, table_rows, timeseries)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Placeholders resulting from previous changes - can be ignored. \n",
    "                    # Separate CV for non time series (Wage Dataset) moved to the create_cv_folds function.\n",
    "                    print(\"set timeseries varible to True\")\n",
    "\n",
    "                # DRF-Modell wird immer ausgeführt\n",
    "                drf_model = DRFWrapper(min_node_size=10, num_trees=100, num_threads=1)\n",
    "                drf_grid = get_grid('DRF', X_train_scaled.shape[1])\n",
    "                evaluate_and_append_models_singleID([('DRF', drf_model, drf_grid)], X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_loss, tau, cu, co, column, table_rows, timeseries)\n",
    "\n",
    "\n",
    "                # Print the table after evaluating each column\n",
    "                second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "                print(second_result_table.tail(5))  \n",
    "\n",
    "\n",
    "# Define the folder where results will be saved\n",
    "results_folder = \"results\"\n",
    "\n",
    "# Check if the results folder exists, if not, create it\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "\n",
    "\n",
    "# safe d\n",
    "\n",
    "filename = os.path.join(results_folder, f\"results_LevelsetModels_{dataset_name}.csv\")\n",
    "second_result_table.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Results saved as {filename}\")\n",
    "\n",
    "\n",
    "if global_fold_scores:  \n",
    "\n",
    "    global_fold_scores_flat = []\n",
    "    for fold_scores_df in global_fold_scores:\n",
    "\n",
    "        flat_df = fold_scores_df.reset_index()\n",
    "        global_fold_scores_flat.append(flat_df)\n",
    "\n",
    "    aggregated_fold_scores_df = pd.concat(global_fold_scores_flat, ignore_index=True)\n",
    "\n",
    "    aggregated_fold_scores_filename = os.path.join(results_folder, f\"cv_scores_levelset_models_{dataset_name}.csv\")\n",
    "    aggregated_fold_scores_df.to_csv(aggregated_fold_scores_filename, index=False)\n",
    "\n",
    "    aggregated_drf_cv_results_df = pd.concat(drf_cv_results, ignore_index=True)\n",
    "\n",
    "    aggregated_cv_filename = os.path.join(results_folder, f\"cv_drf_scores_{dataset_name}.csv\")\n",
    "    aggregated_drf_cv_results_df.to_csv(aggregated_cv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FULL Dataset Cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.shared_imports import *\n",
    "\n",
    "# Verify that the current working directory has changed\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Importiere alle Module\n",
    "from scripts.get_data import get_dataset_settings_alldata, preprocess_data_alldata\n",
    "from Wrapper.wrapper import DRFWrapper, MLPRegressorWrapper\n",
    "from scripts.utils import *\n",
    "from scripts.train_and_evaluate_alldata import (  \n",
    "    evaluate_and_append_models_alldata, create_cv_folds_alldata, preprocess_per_instance_alldata\n",
    ")\n",
    "from scripts.process_target import process_target_alldata\n",
    "\n",
    "import scripts.config as config\n",
    "from scripts.globals import global_fold_scores, global_cv_results, drf_cv_results\n",
    "\n",
    "importlib.reload(config)\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  # OpenMP Threads auf 4 beschränken\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'  # Für OpenBLAS\n",
    "os.environ['MKL_NUM_THREADS'] = '1'  # Für Intel MKL (falls verwendet)\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'  # Für NumExpr\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'  # Für MacOS Accelerate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "dataset_name = config.dataset_name\n",
    "\n",
    "# Hole die Datei-ID für den gewählten Datensatz\n",
    "file_id = {\n",
    "    'subset_bakery': '1r_bDn9Z3Q_XgeTTkJL7352nUG3jkUM0z',\n",
    "    'yaz': '1xrY3Uv5F9F9ofgSM7dVoSK4bE0gPMg36',\n",
    "    'subset_m5': '1tCBaxOgE5HHllvLVeRC18zvALBz6B-6w',\n",
    "    'subset_air': '1DMOaV92n3BFEGeCubaxEys2eLzg2Cic3',\n",
    "    'wage': '1bn7E7NOoRzE4NwXXs1MYhRSKZHC13qYU',\n",
    "}[config.dataset_name]\n",
    "\n",
    "\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "\n",
    "# Datei herunterladen\n",
    "output = f\"{dataset_name}.csv\"\n",
    "gdown.download(url, output, quiet=False)\n",
    "data = pd.read_csv(output)\n",
    "settings = get_dataset_settings_alldata(data)[dataset_name]\n",
    "\n",
    "y, train_data, test_data, X_train_features, X_test_features, y_train, y_test, data, dataset_name  = preprocess_data_alldata(\n",
    "    data=data,\n",
    "    dataset_name=dataset_name,\n",
    "    bool_columns=settings['bool_columns'],\n",
    "    drop_columns=settings['drop_columns'],\n",
    "    drop_keywords=settings['drop_keywords'],\n",
    ")\n",
    "\n",
    "# note: \n",
    "# This is not yet the final data. Data is processed further in the preprocess_per_instance function before the models are trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDOP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "# Execution starts here\n",
    "combinations = [(9, 1), (7.5, 2.5), (5, 5), (2.5, 7.5), (1, 9)]\n",
    "table_rows = []\n",
    "random_state = 1\n",
    "global_cv_results = []\n",
    "\n",
    "# Initialize cvFolds\n",
    "cvFolds = None  # Initialization\n",
    "\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "\n",
    "    column_results = Parallel(n_jobs=1)(\n",
    "        delayed(process_target_alldata)(column, cu, co, tau, y_train, X_train_features, X_test_features, y_test, random_state)\n",
    "        for column in y_train.columns\n",
    "    )\n",
    "\n",
    "    for result in column_results:\n",
    "        table_rows.extend(result)\n",
    "        print(table_rows)\n",
    "        # Convert the latest result to a DataFrame and print it\n",
    "        result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "        print(result_table)  # Print the updated results after each column is processed\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "\n",
    "\n",
    "# safe data\n",
    "filename = f\"FULLDATASET_results_basic_Models_{dataset_name}.csv\"\n",
    "result_table.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Results saved as {filename}\")\n",
    "\n",
    "# Aggregate and save cross-validation results at the end of the entire workflow\n",
    "if global_cv_results:\n",
    "\n",
    "    aggregated_cv_results_df = pd.concat(global_cv_results, ignore_index=True)\n",
    "\n",
    "    aggregated_cv_filename = f\"FULLDATASET_cv_scores_basic_models_{dataset_name}.csv\"\n",
    "    aggregated_cv_results_df.to_csv(aggregated_cv_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRF+Leveset Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = True\n",
    "import scripts.config as config\n",
    "from scripts.config import *\n",
    "importlib.reload(config)\n",
    "\n",
    "combinations = [(9, 1), (7.5, 2.5), (5, 5), (2.5, 7.5), (1, 9)]\n",
    "table_rows = []\n",
    "random_state = 1\n",
    "drf_cv_results = []\n",
    "global_fold_scores = []\n",
    "\n",
    "# Iterate over combinations and process them directly\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "    with threadpool_limits(limits=1):\n",
    "            for column in y_train.columns:\n",
    "                print(f\"Processing column: {column}\")\n",
    "\n",
    "\n",
    "                X_train_scaled, X_test_scaled, y_train_col, y_test_col, X_train_scaled_withID, X_test_scaled_withID = preprocess_per_instance_alldata(\n",
    "                    column, X_train_features, X_test_features, y_train, y_test\n",
    "                )\n",
    "\n",
    "                create_cv_folds_alldata(X_train_scaled_withID)\n",
    "                \n",
    "                # SAA model\n",
    "                saa_model = SampleAverageApproximationNewsvendor(cu, co)\n",
    "                saa_pred = saa_model.fit(y_train_col).predict(X_test_scaled.shape[0])\n",
    "\n",
    "\n",
    "                # Ensure id_for_CV, y_true, and y_pred are 1-D arrays\n",
    "                id_for_CV = X_test_scaled_withID['id_for_CV'].values.flatten()\n",
    "                y_true = y_test_col.values.flatten()\n",
    "                y_pred = saa_pred.flatten()  # Flatten y_pred to ensure it's 1-D\n",
    "\n",
    "                # Create DataFrame for SAA predictions\n",
    "                saa_predictions_df = pd.DataFrame({\n",
    "                    'id_for_CV': id_for_CV,\n",
    "                    'y_true': y_true,\n",
    "                    'y_pred': y_pred  # Use the flattened y_pred here\n",
    "                })\n",
    "\n",
    "                saa_pinball_losses_per_id = {}\n",
    "                grouped_saa = saa_predictions_df.groupby('id_for_CV')\n",
    "                for id_val, group in grouped_saa:\n",
    "                    y_true_id = group['y_true'].values\n",
    "                    y_pred_id = group['y_pred'].values\n",
    "                    pinball_loss_id = pinball_loss(y_true_id, y_pred_id, tau)\n",
    "                    saa_pinball_losses_per_id[id_val] = pinball_loss_id\n",
    "                    append_result(table_rows, id_val, cu, co, 'SAA', pinball_loss_id, 'N/A', np.nan, tau)\n",
    "\n",
    "                n_features = X_train_scaled.shape[1]\n",
    "\n",
    "                drf_model = DRFWrapper(min_node_size=10, num_trees=100, num_threads=1)\n",
    "                drf_grid = get_grid('DRF', X_train_scaled.shape[1])\n",
    "                \n",
    "                print(f\"Length of X_train_scaled: {len(X_train_scaled)}\")\n",
    "                print(f\"Length of X_test_scaled: {len(X_test_scaled)}\")\n",
    "                evaluate_and_append_models_alldata([('DRF', drf_model, drf_grid)], X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_losses_per_id, tau, cu, co, column, table_rows, timeseries, X_test_scaled_withID)\n",
    "\n",
    "\n",
    "            \n",
    "                if timeseries:\n",
    "                    # Initialisiere LGBM und MLP-Modelle\n",
    "                    lgbm_model = LGBMRegressor(random_state=random_state, n_jobs=n_jobs, verbosity=-1)\n",
    "                    mlp_model = MLPRegressorWrapper(random_state=random_state, early_stopping=True)\n",
    "\n",
    "                    # LGBM-Modell mit GroupSplitting evaluieren\n",
    "                    lgbm_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    print(lgbm_model_params)\n",
    "\n",
    "                    lgbm_model_evaluation = [\n",
    "                        ('LS_KDEx_LGBM', LevelSetKDEx(estimator=lgbm_model, binSize=100, weightsByDistance=False), lgbm_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models_alldata(lgbm_model_evaluation, X_train_scaled, X_test_scaled,\n",
    "                                       y_train_col, y_test_col, saa_pinball_losses_per_id,\n",
    "                                       tau, cu, co, column, table_rows, timeseries, X_test_scaled_withID)\n",
    "\n",
    "                    # MLP-Modell mit GroupSplitting evaluieren\n",
    "                    mlp_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    mlp_model_evaluation = [\n",
    "                        ('LS_KDEx_MLP', LevelSetKDEx(estimator=mlp_model, binSize=100, weightsByDistance=False), mlp_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models_alldata(mlp_model_evaluation, X_train_scaled, X_test_scaled,\n",
    "                                       y_train_col, y_test_col, saa_pinball_losses_per_id,\n",
    "                                       tau, cu, co, column, table_rows, timeseries, X_test_scaled_withID)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Placeholders resulting from previous changes - can be ignored. \n",
    "                    # Separate CV for non time series (Wage Dataset) moved to the create_cv_folds function.\n",
    "                    print(\"set timeseries varible to True\")\n",
    "\n",
    "                second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "                print(second_result_table.tail(5))  # Print the last 5 rows of the table after each column is processed\n",
    "\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "\n",
    "\n",
    "# safe data\n",
    "filename = f\"FULLDATASET_results_LevelsetModels_{dataset_name}.csv\"\n",
    "\n",
    "second_result_table.to_csv(filename, index=False)\n",
    "results_folder = \"results\"\n",
    "\n",
    "from scripts.globals import global_fold_scores, global_cv_results, drf_cv_results\n",
    "\n",
    "filename = os.path.join(results_folder, f\"FULLDATASET_results_LevelsetModels_{dataset_name}.csv\")\n",
    "second_result_table.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Results saved as {filename}\")\n",
    "\n",
    "if global_fold_scores: \n",
    "\n",
    "    global_fold_scores_flat = []\n",
    "    for fold_scores_df in global_fold_scores:\n",
    "        \n",
    "        flat_df = fold_scores_df.reset_index()\n",
    "        global_fold_scores_flat.append(flat_df)\n",
    "\n",
    "    aggregated_fold_scores_df = pd.concat(global_fold_scores_flat, ignore_index=True)\n",
    "\n",
    "    aggregated_fold_scores_filename = os.path.join(results_folder, f\"FULLDATASET_cv_scores_levelset_models_{dataset_name}.csv\")\n",
    "    aggregated_fold_scores_df.to_csv(aggregated_fold_scores_filename, index=False)\n",
    "\n",
    "    aggregated_drf_cv_results_df = pd.concat(drf_cv_results, ignore_index=True)\n",
    "\n",
    "    aggregated_cv_filename = os.path.join(results_folder, f\"cv_drf_scores_{dataset_name}.csv\")\n",
    "    aggregated_drf_cv_results_df.to_csv(aggregated_cv_filename, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
