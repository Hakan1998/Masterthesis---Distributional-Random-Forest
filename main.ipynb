{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import gdown\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.rinterface import RRuntimeWarning\n",
    "from rpy2.rinterface_lib.callbacks import logger as rpy2_logger\n",
    "from pulp import LpSolverDefault\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "# Custom or external package imports\n",
    "from ddop2.newsvendor import (\n",
    "    DecisionTreeWeightedNewsvendor, KNeighborsWeightedNewsvendor, \n",
    "    SampleAverageApproximationNewsvendor, DeepLearningNewsvendor, \n",
    "    RandomForestWeightedNewsvendor, GaussianWeightedNewsvendor, \n",
    "    LinearRegressionNewsvendor\n",
    ")\n",
    "from drf import drf\n",
    "from dddex.levelSetKDEx_univariate import LevelSetKDEx\n",
    "from dddex.loadData import loadDataYaz\n",
    "from dddex.crossValidation import QuantileCrossValidation, groupedTimeSeriesSplit\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from threadpoolctl import threadpool_limits  # Importiere threadpool_limits\n",
    "\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column width\n",
    "pd.set_option('display.max_rows', 10)  # Limit the number of displayed rows\n",
    "pd.set_option('display.width', 1000)  # Set high enough width to show all columns in a line\n",
    "\n",
    "# Suppress warnings and logging\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress all Python warnings\n",
    "rpy2_logger.setLevel(logging.CRITICAL)  # Only show critical messages from R\n",
    "\n",
    "# Set R options to suppress warnings and messages\n",
    "ro.r('while (sink.number() > 0) sink(NULL)')  # Close open sinks to avoid \"sink stack full\" errors\n",
    "ro.r('options(warn=-1)')  # Disable all warnings in R\n",
    "ro.r('suppressMessages(suppressWarnings(library(\"drf\")))')  # Suppress R package messages and warnings\n",
    "\n",
    "# Set environment variables for R libraries\n",
    "os.environ['R_LIBS_USER'] = '/usr/lib/R/site-library'\n",
    "os.environ['R_HOME'] = '/usr/lib/R'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# Deactivate CBC Solver output\n",
    "LpSolverDefault.msg = False  # Deactivates the CBC Solver output\n",
    "\n",
    "# Verify that the current working directory has changed\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.get_data import get_dataset_settings, preprocess_data\n",
    "\n",
    "from Wrapper.wrapper import DRFWrapper, MLPRegressorWrapper, LevelSetKDExWrapper\n",
    "\n",
    "from scripts.cv_and_evaluation import pinball_loss, pinball_loss_scorer, get_grid, preprocess_per_instance, train_and_evaluate_model, calculate_n_iter, bayesian_search_model, preprocess_per_instance, append_result, evaluate_and_append_models, create_cv_folds\n",
    "\n",
    "from scripts.process_target import process_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'bakery'  # Hier den Namen des gewünschten Datensatzes b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hole die Datei-ID für den gewählten Datensatz\n",
    "file_id = {\n",
    "    'bakery': '1r_bDn9Z3Q_XgeTTkJL7352nUG3jkUM0z',\n",
    "    'yaz': '1xrY3Uv5F9F9ofgSM7dVoSK4bE0gPMg36',\n",
    "    'm5': '1tCBaxOgE5HHllvLVeRC18zvALBz6B-6w',\n",
    "    'sid': '1J9bPCfeLDH-mbSnvTHRoCva7pl6cXD3_',\n",
    "    'air': '1SKPpNxulcusNTjRwCC0p3C_XW7aNBNJZ',\n",
    "    \"copula\": '1H5wdJgmxdhbzeS17w0NkRlHRCESEAd-e',\n",
    "    'wage': '1bn7E7NOoRzE4NwXXs1MYhRSKZHC13qYU',\n",
    "}[dataset_name]\n",
    "\n",
    "\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "\n",
    "# Datei herunterladen\n",
    "output = f\"{dataset_name}.csv\"\n",
    "gdown.download(url, output, quiet=False)\n",
    "data = pd.read_csv(output)\n",
    "\n",
    "# Erstelle die Dataset-Einstellungen basierend auf den geladenen Daten\n",
    "settings = get_dataset_settings(data)[dataset_name]\n",
    "\n",
    "y, train_data, test_data, X_train_features, X_test_features, y_train, y_test = preprocess_data(\n",
    "    data, settings['backscaling_columns'], settings['bool_columns'], settings['drop_columns'])\n",
    "\n",
    "\n",
    "display(X_train_features.head(10))\n",
    "display(y_train.head(3))\n",
    "\n",
    "print(\"Anzahl der targets:\", len(y_train.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OMP_NUM_THREADS'] = '1'  # OpenMP Threads auf 4 beschränken\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'  # Für OpenBLAS\n",
    "os.environ['MKL_NUM_THREADS'] = '1'  # Für Intel MKL (falls verwendet)\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'  # Für NumExpr\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'  # Für MacOS Accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Execution starts here\n",
    "combinations = [(9, 1), (7.5, 2.5), (5, 5), (2.5, 7.5), (1, 9)]\n",
    "table_rows = []\n",
    "random_state = 42\n",
    "\n",
    "# Initialize cvFolds\n",
    "cvFolds = None  # Initialization\n",
    "\n",
    "\n",
    "import scripts.globals as globals  # Import the globals module\n",
    "\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "\n",
    "    # Parallelize column processing within each combination with n_jobs=4 to limit threads\n",
    "    column_results = Parallel(n_jobs=1)(  \n",
    "        delayed(process_column)(column, cu, co, tau, y_train, X_train_features, X_test_features, y_test, random_state)\n",
    "        for column in y_train.columns\n",
    "    )\n",
    "\n",
    "    # Combine results from all columns and print after each column\n",
    "    for result in column_results:\n",
    "        table_rows.extend(result)\n",
    "        print(table_rows)\n",
    "        # Convert the latest result to a DataFrame and print it\n",
    "        result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "        print(result_table)  # Print the updated results after each column is processed\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "\n",
    "# Construct the filename using the format \"results_basicModels_{dataset_name}.csv\"\n",
    "filename = f\"results_basic_Models_{dataset_name}.csv\"\n",
    "\n",
    "# Save the result table to a CSV file\n",
    "result_table.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Results saved as {filename}\")\n",
    "\n",
    "# Aggregate and save cross-validation results at the end of the entire workflow\n",
    "if globals.global_cv_results:\n",
    "    # Concatenate all cross-validation results into a single DataFrame\n",
    "    aggregated_cv_results_df = pd.concat(globals.global_cv_results, ignore_index=True)\n",
    "\n",
    "    # Print a summary of the aggregated cross-validation data to verify it looks correct\n",
    "    print(\"Aggregated cross-validation results sample:\")\n",
    "    print(aggregated_cv_results_df.head(5))  # Print the first 5 rows as a sample\n",
    "\n",
    "    # Save the aggregated results to a CSV file\n",
    "    aggregated_cv_filename = f\"cv_scores_basic_models_{dataset_name}.csv\"\n",
    "    aggregated_cv_results_df.to_csv(aggregated_cv_filename, index=False)\n",
    "    print(f\"Aggregated cross-validation results saved as {aggregated_cv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter out both 'SAA' and 'LinearRegression' models\n",
    "filtered_table = result_table[(~result_table['Model'].isin(['SAA', 'LR', 'MLP','LGBM'])) & (result_table['sl'] == 0.9)]\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the boxplot\n",
    "sns.boxplot(x='Model', y='delta C', data=filtered_table, showfliers=False, width=0.5, color='lightblue')\n",
    "\n",
    " #Add the stripplot to show the individual data points\n",
    "#sns.stripplot(x='Model', y='delta C', data=filtered_table, color='red', jitter=True, size=6, alpha=0.7)\n",
    "\n",
    "# Add the point plot to show CI based on SD without horizontal lines\n",
    "sns.pointplot(x=\"Model\", y=\"delta C\", data=filtered_table, ci='sd', color='blue', markers=\"o\", scale=0.7, linestyles=\"\")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Boxplot, Stripplot, and Pointplot (CI=SD) of Delta C for Each Model (Excluding SAA and LinearRegression)', fontsize=14)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Delta C', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from collections import OrderedDict\n",
    "\n",
    "timeseries = True\n",
    "\n",
    "# Execution starts here\n",
    "combinations = [(9, 1), (7.5, 2.5), (5, 5), (2.5, 7.5), (1, 9)]\n",
    "table_rows = []\n",
    "random_state = 1\n",
    "drf_cv_results = []\n",
    "global_fold_scores = []\n",
    "\n",
    "# Iterate over combinations and process them directly\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "    with threadpool_limits(limits=10):\n",
    "            for column in y_train.columns:\n",
    "                print(f\"Processing column: {column}\")\n",
    "\n",
    "                # Preprocess data\n",
    "                X_train_scaled, X_test_scaled, y_train_col, y_test_col, X_train_scaled_withID = preprocess_per_instance(\n",
    "                    column, X_train_features, X_test_features, y_train, y_test\n",
    "                )\n",
    "                create_cv_folds(X_train_scaled_withID)\n",
    "                \n",
    "                # SAA model evaluation\n",
    "                saa_model = SampleAverageApproximationNewsvendor(cu, co)\n",
    "                saa_pred = saa_model.fit(y_train_col).predict(X_test_scaled.shape[0])\n",
    "                saa_pinball_loss = pinball_loss(y_test_col.values.flatten(), saa_pred, tau)\n",
    "                append_result(table_rows, column, cu, co, 'SAA', saa_pinball_loss, 'N/A', np.nan, tau)\n",
    "\n",
    "            \n",
    "            \n",
    "                if timeseries:\n",
    "                    # Initialisiere LGBM und MLP-Modelle\n",
    "                    lgbm_model = LGBMRegressor(random_state=random_state, n_jobs=1, verbosity=-1)\n",
    "                    mlp_model = MLPRegressorWrapper(random_state=random_state, early_stopping=True)\n",
    "\n",
    "                    # LGBM-Modell mit GroupSplitting evaluieren\n",
    "                    lgbm_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    lgbm_model_evaluation = [\n",
    "                        ('LS_KDEx_LGBM', LevelSetKDEx(estimator=lgbm_model, binSize=100, weightsByDistance=False), lgbm_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models(lgbm_model_evaluation, X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_loss, tau, cu, co, column, table_rows, timeseries)\n",
    "\n",
    "                    # MLP-Modell mit GroupSplitting evaluieren\n",
    "                    mlp_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    mlp_model_evaluation = [\n",
    "                        ('LS_KDEx_MLP', LevelSetKDEx(estimator=mlp_model, binSize=100, weightsByDistance=False), mlp_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models(mlp_model_evaluation, X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_loss, tau, cu, co, column, table_rows, timeseries)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Although the Wage data set is not a timeseries, it works similarly well on group timeseries splits. \n",
    "                    # To do this, we set shuffle = True once in the train/test split. \n",
    "                    # Resulting in an mixed order of the train/test points, even if we order it by the Index.\n",
    "                    print(\"Only time series Data\")\n",
    "\n",
    "                # DRF-Modell wird immer ausgeführt\n",
    "                drf_model = DRFWrapper(min_node_size=10, num_trees=100, num_threads=1)\n",
    "                drf_grid = get_grid('DRF', X_train_scaled.shape[1])\n",
    "                evaluate_and_append_models([('DRF', drf_model, drf_grid)], X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_loss, tau, cu, co, column, table_rows, timeseries)\n",
    "\n",
    "\n",
    "                # Print the table after evaluating each column\n",
    "                second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "                print(second_result_table.tail(5))  # Print the last 5 rows of the table after each column is processed\n",
    "\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "filename = f\"results_LevelsetModels_{dataset_name}.csv\"\n",
    "\n",
    "\n",
    "second_result_table.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Results saved as {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "if globals.global_fold_scores:\n",
    "    # Reset multi-index for all fold scores\n",
    "    global_fold_scores_flat = []\n",
    "    for fold_scores_df in globals.global_fold_scores:\n",
    "        # Reset the multi-index so that the binSize and weightsByDistance become normal columns\n",
    "        flat_df = fold_scores_df.reset_index()\n",
    "        global_fold_scores_flat.append(flat_df)\n",
    "\n",
    "    # Concatenate all fold-wise cross-validation results into a single DataFrame\n",
    "    aggregated_fold_scores_df = pd.concat(global_fold_scores_flat, ignore_index=True)\n",
    "\n",
    "    # Print a summary of the aggregated fold-wise data to verify it looks correct\n",
    "    print(\"Aggregated fold-wise cross-validation results sample:\")\n",
    "    print(aggregated_fold_scores_df)  # Print the first 5 rows as a sample\n",
    "\n",
    "    # Save the aggregated results to a CSV file\n",
    "    aggregated_fold_scores_filename = f\"cv_scores_levelset_models_{dataset_name}.csv\"\n",
    "    aggregated_fold_scores_df.to_csv(aggregated_fold_scores_filename, index=False)\n",
    "\n",
    "    print(f\"Aggregated fold-wise cross-validation results saved as {aggregated_fold_scores_filename}\")\n",
    "\n",
    "    ### DRF DATEN ZUSÄTZLICH IN DIE EIGENTLICHE TABELLE EINFÜGEN, IN DER SICH DIE ANDEREN BAYES CV befinden\n",
    "\n",
    "\n",
    "    aggregated_drf_cv_results_df = pd.concat(globals.drf_cv_results, ignore_index=True)\n",
    "\n",
    "    # Print a summary of the aggregated cross-validation data to verify it looks correct\n",
    "    print(\"Aggregated cross-validation results sample:\")\n",
    "    print(aggregated_drf_cv_results_df.head(5))  # Print the first 5 rows as a sample\n",
    "\n",
    "    # Save the aggregated results to a CSV file\n",
    "    aggregated_cv_filename = f\"cv_drf_scores_{dataset_name}.csv\"\n",
    "    aggregated_drf_cv_results_df.to_csv(aggregated_cv_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_enviroment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
