{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single ID Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.shared_imports import *\n",
    "\n",
    "# Verify that the current working directory has changed\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Importiere alle Module\n",
    "from scripts.get_data import get_dataset_settings_singleID, preprocess_data_singleID\n",
    "from Wrapper.wrapper import DRFWrapper, MLPRegressorWrapper\n",
    "from scripts.utils import *\n",
    "from scripts.train_and_evaluate_singleID import (  \n",
    "    append_result, evaluate_and_append_models_singleID, create_cv_folds_singleID,preprocess_per_instance_singleID\n",
    ")\n",
    "from scripts.process_target import process_target_singleID\n",
    "\n",
    "import scripts.config as config\n",
    "from scripts.globals import global_fold_scores, global_cv_results, drf_cv_results\n",
    "\n",
    "# Lade alle Module neu\n",
    "importlib.reload(config)\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  # OpenMP Threads auf 4 beschränken\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'  # Für OpenBLAS\n",
    "os.environ['MKL_NUM_THREADS'] = '1'  # Für Intel MKL (falls verwendet)\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'  # Für NumExpr\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'  # Für MacOS Accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set configurations in config.py file before we start process\n",
    "\n",
    "--> set dataset_name before in config file\n",
    "\n",
    "--> set levelset_calcuations to False if we do the basic models calcuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = config.dataset_name\n",
    "\n",
    "# Hole die Datei-ID für den gewählten Datensatz\n",
    "file_id = {\n",
    "    'bakery': '1r_bDn9Z3Q_XgeTTkJL7352nUG3jkUM0z',\n",
    "    'yaz': '1xrY3Uv5F9F9ofgSM7dVoSK4bE0gPMg36',\n",
    "    'm5': '1tCBaxOgE5HHllvLVeRC18zvALBz6B-6w',\n",
    "    'sid': '1J9bPCfeLDH-mbSnvTHRoCva7pl6cXD3_',\n",
    "    'air': '1SKPpNxulcusNTjRwCC0p3C_XW7aNBNJZ',\n",
    "    \"copula\": '1H5wdJgmxdhbzeS17w0NkRlHRCESEAd-e',\n",
    "    'wage': '1bn7E7NOoRzE4NwXXs1MYhRSKZHC13qYU',\n",
    "}[config.dataset_name]\n",
    "\n",
    "\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "\n",
    "# Datei herunterladen\n",
    "output = f\"{dataset_name}.csv\"\n",
    "gdown.download(url, output, quiet=False)\n",
    "data = pd.read_csv(output)\n",
    "\n",
    "# Erstelle die Dataset-Einstellungen basierend auf den geladenen Daten\n",
    "settings = get_dataset_settings_singleID(data)[dataset_name]\n",
    "\n",
    "y, train_data, test_data, X_train_features, X_test_features, y_train, y_test = preprocess_data_singleID(\n",
    "    data, settings['backscaling_columns'], settings['bool_columns'], settings['drop_columns'])\n",
    "\n",
    "\n",
    "display(X_train_features.head(30))\n",
    "display(y_train.head(3))\n",
    "print(f\"Anzahl der Zeilen: {len(y_train)}\")\n",
    "print(\"Anzahl der targets:\", len(y_train.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process - DDOP Models + Levelset Estimator Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Execution starts here\n",
    "combinations = [(9, 1), (7.5, 2.5), (5, 5), (2.5, 7.5), (1, 9)]\n",
    "table_rows = []\n",
    "random_state = 42\n",
    "\n",
    "# Initialize cvFolds\n",
    "cvFolds = None  # Initialization\n",
    "\n",
    "\n",
    "import scripts.globals as globals  # Import the globals module\n",
    "\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "\n",
    "    # Parallelize column processing within each combination with n_jobs=4 to limit threads\n",
    "    column_results = Parallel(n_jobs=1)(  \n",
    "        delayed(process_target_singleID)(column, cu, co, tau, y_train, X_train_features, X_test_features, y_test, random_state)\n",
    "        for column in y_train.columns\n",
    "    )\n",
    "\n",
    "    # Combine results from all columns and print after each column\n",
    "    for result in column_results:\n",
    "        table_rows.extend(result)\n",
    "        print(table_rows)\n",
    "        # Convert the latest result to a DataFrame and print it\n",
    "        result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "        print(result_table)  # Print the updated results after each column is processed\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "\n",
    "# Define the folder where results will be saved\n",
    "results_folder = \"results\"\n",
    "\n",
    "# Check if the results folder exists, if not, create it\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "# Construct the filename using the format \"results_basic_Models_{dataset_name}.csv\"\n",
    "filename = os.path.join(results_folder, f\"results_basic_Models_{dataset_name}.csv\")\n",
    "\n",
    "# Save the result table to a CSV file in the \"results\" folder\n",
    "result_table.to_csv(filename, index=False)\n",
    "\n",
    "# Aggregate and save cross-validation results at the end of the entire workflow\n",
    "if globals.global_cv_results:\n",
    "    # Concatenate all cross-validation results into a single DataFrame\n",
    "    aggregated_cv_results_df = pd.concat(globals.global_cv_results, ignore_index=True)\n",
    "\n",
    "    # Save the aggregated results to a CSV file in the \"results\" folder\n",
    "    aggregated_cv_filename = os.path.join(results_folder, f\"cv_scores_basic_models_{dataset_name}.csv\")\n",
    "    aggregated_cv_results_df.to_csv(aggregated_cv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter out both 'SAA' and 'LinearRegression' models\n",
    "filtered_table = result_table[(~result_table['Model'].isin(['SAA', 'LR', 'MLP','LGBM'])) & (result_table['sl'] == 0.9)]\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the boxplot\n",
    "sns.boxplot(x='Model', y='delta C', data=filtered_table, showfliers=False, width=0.5, color='lightblue')\n",
    "\n",
    " #Add the stripplot to show the individual data points\n",
    "#sns.stripplot(x='Model', y='delta C', data=filtered_table, color='red', jitter=True, size=6, alpha=0.7)\n",
    "\n",
    "# Add the point plot to show CI based on SD without horizontal lines\n",
    "sns.pointplot(x=\"Model\", y=\"delta C\", data=filtered_table, ci='sd', color='blue', markers=\"o\", scale=0.7, linestyles=\"\")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Boxplot, Stripplot, and Pointplot (CI=SD) of Delta C for Each Model (Excluding SAA and LinearRegression)', fontsize=14)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Delta C', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process - DRF + Levelset Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "\n",
    "timeseries = True\n",
    "\n",
    "# Execution starts here\n",
    "combinations = [(1, 9)]\n",
    "table_rows = []\n",
    "random_state = 1\n",
    "drf_cv_results = []\n",
    "global_fold_scores = []\n",
    "\n",
    "# Iterate over combinations and process them directly\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "    with threadpool_limits(limits=1):\n",
    "            for column in y_train.columns:\n",
    "                print(f\"Processing column: {column}\")\n",
    "\n",
    "                # Preprocess data\n",
    "                X_train_scaled, X_test_scaled, y_train_col, y_test_col, X_train_scaled_withID = preprocess_per_instance_singleID(\n",
    "                    column, X_train_features, X_test_features, y_train, y_test\n",
    "                )\n",
    "                create_cv_folds_singleID(X_train_scaled_withID)\n",
    "            \n",
    "\n",
    "                # SAA model evaluation\n",
    "                saa_model = SampleAverageApproximationNewsvendor(cu, co)\n",
    "                saa_pred = saa_model.fit(y_train_col).predict(X_test_scaled.shape[0])\n",
    "                saa_pinball_loss = pinball_loss(y_test_col.values.flatten(), saa_pred, tau)\n",
    "                append_result(table_rows, column, cu, co, 'SAA', saa_pinball_loss, 'N/A', np.nan, tau)\n",
    "\n",
    "            \n",
    "            \n",
    "                if timeseries:\n",
    "                    # Initialisiere LGBM und MLP-Modelle\n",
    "                    lgbm_model = LGBMRegressor(random_state=random_state, n_jobs=1, verbosity=-1)\n",
    "                    mlp_model = MLPRegressorWrapper(random_state=random_state, early_stopping=True)\n",
    "\n",
    "                    # LGBM-Modell mit GroupSplitting evaluieren\n",
    "                    lgbm_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    lgbm_model_evaluation = [\n",
    "                        ('LS_KDEx_LGBM', LevelSetKDEx(estimator=lgbm_model, binSize=100, weightsByDistance=False), lgbm_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models_singleID(lgbm_model_evaluation, X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_loss, tau, cu, co, column, table_rows, timeseries)\n",
    "\n",
    "                    # MLP-Modell mit GroupSplitting evaluieren\n",
    "                    mlp_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    mlp_model_evaluation = [\n",
    "                        ('LS_KDEx_MLP', LevelSetKDEx(estimator=mlp_model, binSize=100, weightsByDistance=False), mlp_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models_singleID(mlp_model_evaluation, X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_loss, tau, cu, co, column, table_rows, timeseries)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Although the Wage data set is not a timeseries, it works similarly well on group timeseries splits. \n",
    "                    # To do this, we set shuffle = True beforehand in the preprocessing for train/test split. \n",
    "                    # Resulting in an mixed order of the train/test points, even if we order it by the \"Dayindex\" later in the CV splits.\n",
    "                    # saves the extra work and we can work the same split logic for all datasets\n",
    "                    print(\"Only time series Data\")\n",
    "\n",
    "                # DRF-Modell wird immer ausgeführt\n",
    "                drf_model = DRFWrapper(min_node_size=10, num_trees=100, num_threads=12)\n",
    "                drf_grid = get_grid('DRF', X_train_scaled.shape[1])\n",
    "                evaluate_and_append_models_singleID([('DRF', drf_model, drf_grid)], X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_loss, tau, cu, co, column, table_rows, timeseries)\n",
    "\n",
    "\n",
    "                # Print the table after evaluating each column\n",
    "                second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "                print(second_result_table.tail(5))  # Print the last 5 rows of the table after each column is processed\n",
    "\n",
    "\n",
    "# Define the folder where results will be saved\n",
    "results_folder = \"results\"\n",
    "\n",
    "# Check if the results folder exists, if not, create it\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "\n",
    "\n",
    "# Construct the filename and save it in the \"results\" folder\n",
    "filename = os.path.join(results_folder, f\"results_LevelsetModels_{dataset_name}.csv\")\n",
    "second_result_table.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Results saved as {filename}\")\n",
    "\n",
    "# Aggregating fold-wise cross-validation results\n",
    "if global_fold_scores:  # No 'globals' function, just directly use 'global_fold_scores'\n",
    "    # Reset multi-index for all fold scores\n",
    "    global_fold_scores_flat = []\n",
    "    for fold_scores_df in global_fold_scores:\n",
    "        # Reset the multi-index so that the binSize and weightsByDistance become normal columns\n",
    "        flat_df = fold_scores_df.reset_index()\n",
    "        global_fold_scores_flat.append(flat_df)\n",
    "\n",
    "    # Concatenate all fold-wise cross-validation results into a single DataFrame\n",
    "    aggregated_fold_scores_df = pd.concat(global_fold_scores_flat, ignore_index=True)\n",
    "\n",
    "    # Save the aggregated results to a CSV file in the \"results\" folder\n",
    "    aggregated_fold_scores_filename = os.path.join(results_folder, f\"cv_scores_levelset_models_{dataset_name}.csv\")\n",
    "    aggregated_fold_scores_df.to_csv(aggregated_fold_scores_filename, index=False)\n",
    "\n",
    "    ### DRF DATA INSERTED INTO THE MAIN TABLE WHERE OTHER BAYES CVs ARE STORED\n",
    "    aggregated_drf_cv_results_df = pd.concat(drf_cv_results, ignore_index=True)\n",
    "\n",
    "    # Save the aggregated DRF results to a CSV file in the \"results\" folder\n",
    "    aggregated_cv_filename = os.path.join(results_folder, f\"cv_drf_scores_{dataset_name}.csv\")\n",
    "    aggregated_drf_cv_results_df.to_csv(aggregated_cv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter out specific models and only include rows where 'sl' equals 0.9\n",
    "filtered_table = second_result_table[\n",
    "    (~second_result_table['Model'].isin(['SAA', 'LR', 'MLP', 'LGBM'])) & \n",
    "    (second_result_table['sl'] == 0.9)\n",
    "]\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the boxplot\n",
    "sns.boxplot(x='Model', y='delta C', data=filtered_table, showfliers=False, width=0.5, color='lightblue')\n",
    "\n",
    "# Add the point plot to show CI based on SD without horizontal lines\n",
    "sns.pointplot(x=\"Model\", y=\"delta C\", data=filtered_table, ci='sd', color='blue', markers=\"o\", scale=0.7, linestyles=\"\")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Boxplot and Pointplot (CI=SD) of Delta C for Each Model (Excluding SAA and LinearRegression)', fontsize=14)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Delta C', fontsize=12)\n",
    "\n",
    "# Show the plot with tight layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FULL Dataset Cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 18:56:57.843138: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-19 18:56:57.845985: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-19 18:56:57.890051: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-19 18:56:57.893621: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-19 18:56:59.751773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /root/WorkingFolder\n"
     ]
    }
   ],
   "source": [
    "from scripts.shared_imports import *\n",
    "\n",
    "# Verify that the current working directory has changed\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Importiere alle Module\n",
    "from scripts.get_data import get_dataset_settings_alldata, preprocess_data_alldata\n",
    "from Wrapper.wrapper import DRFWrapper, MLPRegressorWrapper\n",
    "from scripts.utils import *\n",
    "from scripts.train_and_evaluate_alldata import (  \n",
    "    evaluate_and_append_models_alldata, create_cv_folds_alldata, preprocess_per_instance_alldata\n",
    ")\n",
    "from scripts.process_target import process_target_alldata\n",
    "\n",
    "import scripts.config as config\n",
    "from scripts.globals import global_fold_scores, global_cv_results, drf_cv_results\n",
    "\n",
    "# Lade alle Module neu\n",
    "importlib.reload(config)\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  # OpenMP Threads auf 4 beschränken\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'  # Für OpenBLAS\n",
    "os.environ['MKL_NUM_THREADS'] = '1'  # Für Intel MKL (falls verwendet)\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'  # Für NumExpr\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'  # Für MacOS Accelerate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1bn7E7NOoRzE4NwXXs1MYhRSKZHC13qYU\n",
      "To: /root/WorkingFolder/wage.csv\n",
      "100%|██████████| 3.25M/3.25M [00:00<00:00, 29.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnung: sample_size (50) ist größer als die Anzahl der verfügbaren eindeutigen IDs (6). Die Stichprobengröße wird auf 6 reduziert.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demand</th>\n",
       "      <th>dayIndex</th>\n",
       "      <th>label</th>\n",
       "      <th>age</th>\n",
       "      <th>citizenship</th>\n",
       "      <th>family_size</th>\n",
       "      <th>children</th>\n",
       "      <th>english_level</th>\n",
       "      <th>race_AIAN</th>\n",
       "      <th>race_asian</th>\n",
       "      <th>race_black</th>\n",
       "      <th>race_mix</th>\n",
       "      <th>race_NHOPI</th>\n",
       "      <th>race_other</th>\n",
       "      <th>race_white</th>\n",
       "      <th>hispanic_origin_no</th>\n",
       "      <th>hispanic_origin_yes</th>\n",
       "      <th>nativity_foreign-born</th>\n",
       "      <th>nativity_native</th>\n",
       "      <th>marital_divorced</th>\n",
       "      <th>marital_married</th>\n",
       "      <th>marital_never married</th>\n",
       "      <th>marital_separated</th>\n",
       "      <th>marital_widowed</th>\n",
       "      <th>employer_for-profit company</th>\n",
       "      <th>employer_government</th>\n",
       "      <th>employer_non-profit company</th>\n",
       "      <th>employer_self-employed</th>\n",
       "      <th>economic_region_Abroad</th>\n",
       "      <th>economic_region_Far West</th>\n",
       "      <th>economic_region_Great Lakes</th>\n",
       "      <th>economic_region_Mideast</th>\n",
       "      <th>economic_region_New England</th>\n",
       "      <th>economic_region_Plains</th>\n",
       "      <th>economic_region_Rocky Mountain</th>\n",
       "      <th>economic_region_Southeast</th>\n",
       "      <th>economic_region_Southwest</th>\n",
       "      <th>occupation_11</th>\n",
       "      <th>occupation_13</th>\n",
       "      <th>occupation_15</th>\n",
       "      <th>occupation_17</th>\n",
       "      <th>occupation_19</th>\n",
       "      <th>occupation_21</th>\n",
       "      <th>occupation_23</th>\n",
       "      <th>occupation_25</th>\n",
       "      <th>occupation_27</th>\n",
       "      <th>occupation_29</th>\n",
       "      <th>occupation_31</th>\n",
       "      <th>occupation_33</th>\n",
       "      <th>occupation_35</th>\n",
       "      <th>occupation_37</th>\n",
       "      <th>occupation_39</th>\n",
       "      <th>occupation_41</th>\n",
       "      <th>occupation_43</th>\n",
       "      <th>occupation_45</th>\n",
       "      <th>occupation_47</th>\n",
       "      <th>occupation_49</th>\n",
       "      <th>occupation_51</th>\n",
       "      <th>occupation_53</th>\n",
       "      <th>occupation_55</th>\n",
       "      <th>industry_11</th>\n",
       "      <th>industry_21</th>\n",
       "      <th>industry_22</th>\n",
       "      <th>industry_23</th>\n",
       "      <th>industry_31</th>\n",
       "      <th>industry_41</th>\n",
       "      <th>industry_44</th>\n",
       "      <th>industry_48</th>\n",
       "      <th>industry_51</th>\n",
       "      <th>industry_52</th>\n",
       "      <th>industry_53</th>\n",
       "      <th>industry_54</th>\n",
       "      <th>industry_55</th>\n",
       "      <th>industry_56</th>\n",
       "      <th>industry_61</th>\n",
       "      <th>industry_62</th>\n",
       "      <th>industry_71</th>\n",
       "      <th>industry_72</th>\n",
       "      <th>industry_81</th>\n",
       "      <th>industry_91</th>\n",
       "      <th>male</th>\n",
       "      <th>id_for_CV</th>\n",
       "      <th>dummyID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.406497</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>dummyID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.322788</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>dummyID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.035106</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>dummyID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.436116</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>dummyID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.035106</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>dummyID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17363</th>\n",
       "      <td>2.322788</td>\n",
       "      <td>2133</td>\n",
       "      <td>train</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>dummyID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17364</th>\n",
       "      <td>3.837915</td>\n",
       "      <td>2134</td>\n",
       "      <td>train</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>dummyID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17366</th>\n",
       "      <td>3.781403</td>\n",
       "      <td>2136</td>\n",
       "      <td>train</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>dummyID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17368</th>\n",
       "      <td>3.239079</td>\n",
       "      <td>2138</td>\n",
       "      <td>train</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>dummyID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17369</th>\n",
       "      <td>3.239079</td>\n",
       "      <td>2139</td>\n",
       "      <td>train</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>dummyID</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13894 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         demand  dayIndex  label  age  citizenship  family_size  children  english_level  race_AIAN  race_asian  race_black  race_mix  race_NHOPI  race_other  race_white  hispanic_origin_no  hispanic_origin_yes  nativity_foreign-born  nativity_native  marital_divorced  marital_married  marital_never married  marital_separated  marital_widowed  employer_for-profit company  employer_government  employer_non-profit company  employer_self-employed  economic_region_Abroad  economic_region_Far West  economic_region_Great Lakes  economic_region_Mideast  economic_region_New England  economic_region_Plains  economic_region_Rocky Mountain  economic_region_Southeast  economic_region_Southwest  occupation_11  occupation_13  occupation_15  occupation_17  occupation_19  occupation_21  occupation_23  occupation_25  occupation_27  occupation_29  occupation_31  occupation_33  occupation_35  occupation_37  occupation_39  occupation_41  occupation_43  occupation_45  occupation_47  occupation_49  \\\n",
       "0      1.406497         1  train   17            1            2         1              0          0           0           0         0           0           0           1                   1                    0                      0                1                 0                0                      1                  0                0                            1                    0                            0                       0                       0                         0                            0                        0                            0                       0                               0                          1                          0              0              0              0              0              0              0              0              0              0              0              0              0              1              0              0              0              0              0              0              0   \n",
       "1      2.322788         2  train   18            1            4         0              0          0           0           0         0           0           0           1                   1                    0                      0                1                 0                0                      1                  0                0                            1                    0                            0                       0                       0                         0                            0                        0                            0                       0                               0                          1                          0              0              0              0              0              0              0              0              0              0              0              0              0              1              0              0              0              0              0              0              0   \n",
       "2      2.035106         3  train   18            1            6         2              0          0           0           1         0           0           0           0                   1                    0                      0                1                 0                0                      1                  0                0                            1                    0                            0                       0                       0                         0                            0                        0                            0                       0                               0                          1                          0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0   \n",
       "3      2.436116         4  train   18            1            4         0              0          0           0           1         0           0           0           0                   1                    0                      0                1                 0                0                      1                  0                0                            1                    0                            0                       0                       0                         0                            0                        0                            0                       0                               1                          0                          0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0   \n",
       "4      2.035106         5  train   18            1            2         0              0          0           0           0         0           0           0           1                   1                    0                      0                1                 0                0                      1                  0                0                            1                    0                            0                       0                       0                         0                            0                        1                            0                       0                               0                          0                          0              0              0              0              0              0              0              0              1              0              0              0              0              0              0              0              0              0              0              0              0   \n",
       "...         ...       ...    ...  ...          ...          ...       ...            ...        ...         ...         ...       ...         ...         ...         ...                 ...                  ...                    ...              ...               ...              ...                    ...                ...              ...                          ...                  ...                          ...                     ...                     ...                       ...                          ...                      ...                          ...                     ...                             ...                        ...                        ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...            ...   \n",
       "17363  2.322788      2133  train   77            1            1         0              1          0           0           0         0           0           0           1                   1                    0                      0                1                 1                0                      0                  0                0                            1                    0                            0                       0                       0                         0                            0                        0                            0                       0                               0                          1                          0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              1              0              0              0   \n",
       "17364  3.837915      2134  train   77            1            1         0              0          0           0           0         0           0           0           1                   1                    0                      0                1                 1                0                      0                  0                0                            0                    0                            1                       0                       0                         1                            0                        0                            0                       0                               0                          0                          0              1              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0   \n",
       "17366  3.781403      2136  train   78            1            2         0              0          0           0           0         0           0           0           1                   1                    0                      0                1                 0                1                      0                  0                0                            1                    0                            0                       0                       0                         0                            0                        1                            0                       0                               0                          0                          0              1              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0   \n",
       "17368  3.239079      2138  train   80            1            2         0              0          0           0           0         0           0           0           1                   1                    0                      0                1                 0                1                      0                  0                0                            1                    0                            0                       0                       0                         0                            0                        0                            0                       0                               0                          1                          0              0              0              0              0              0              0              0              1              0              0              0              0              0              0              0              0              0              0              0              0   \n",
       "17369  3.239079      2139  train   86            4            3         0              1          0           0           0         0           0           0           1                   1                    0                      1                0                 0                1                      0                  0                0                            1                    0                            0                       0                       0                         0                            1                        0                            0                       0                               0                          0                          0              0              1              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0   \n",
       "\n",
       "       occupation_51  occupation_53  occupation_55  industry_11  industry_21  industry_22  industry_23  industry_31  industry_41  industry_44  industry_48  industry_51  industry_52  industry_53  industry_54  industry_55  industry_56  industry_61  industry_62  industry_71  industry_72  industry_81  industry_91  male  id_for_CV  dummyID  \n",
       "0                  0              0              0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            1            0            0     0         16  dummyID  \n",
       "1                  0              0              0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            1            0            0     1         16  dummyID  \n",
       "2                  0              1              0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            1            0     1         16  dummyID  \n",
       "3                  0              1              0            0            0            0            0            1            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0     1         16  dummyID  \n",
       "4                  0              0              0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            1            0            0            0            0            0     0         16  dummyID  \n",
       "...              ...            ...            ...          ...          ...          ...          ...          ...          ...          ...          ...          ...          ...          ...          ...          ...          ...          ...          ...          ...          ...          ...          ...   ...        ...      ...  \n",
       "17363              0              0              0            0            0            0            0            0            0            0            0            0            0            0            0            0            1            0            0            0            0            0            0     0         22  dummyID  \n",
       "17364              0              0              0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            1            0            0            0            0     0         22  dummyID  \n",
       "17366              0              0              0            0            0            0            0            1            0            0            0            0            0            0            0            0            0            0            0            0            0            0            0     0         22  dummyID  \n",
       "17368              0              0              0            0            0            0            0            0            0            0            0            0            0            0            0            0            0            1            0            0            0            0            0     1         22  dummyID  \n",
       "17369              0              0              0            0            0            0            0            0            0            1            0            0            0            0            0            0            0            0            0            0            0            0            0     1         22  dummyID  \n",
       "\n",
       "[13894 rows x 83 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummyID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_for_CV</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.406497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.322788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.035106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dummyID\n",
       "id_for_CV          \n",
       "16         1.406497\n",
       "16         2.322788\n",
       "16         2.035106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Zeilen: 13894\n",
      "Anzahl der targets: 1\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "\n",
    "# Hole die Datei-ID für den gewählten Datensatz\n",
    "file_id = {\n",
    "    'bakery': '1r_bDn9Z3Q_XgeTTkJL7352nUG3jkUM0z',\n",
    "    'yaz': '1xrY3Uv5F9F9ofgSM7dVoSK4bE0gPMg36',\n",
    "    'm5': '1tCBaxOgE5HHllvLVeRC18zvALBz6B-6w',\n",
    "    'sid': '1J9bPCfeLDH-mbSnvTHRoCva7pl6cXD3_',\n",
    "    'air': '1DMOaV92n3BFEGeCubaxEys2eLzg2Cic3',\n",
    "    \"copula\": '1H5wdJgmxdhbzeS17w0NkRlHRCESEAd-e',\n",
    "    'wage': '1bn7E7NOoRzE4NwXXs1MYhRSKZHC13qYU',\n",
    "}[config.dataset_name]\n",
    "\n",
    "\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "\n",
    "# Datei herunterladen\n",
    "output = f\"{dataset_name}.csv\"\n",
    "gdown.download(url, output, quiet=False)\n",
    "data = pd.read_csv(output)\n",
    "\n",
    "# Erstelle die Dataset-Einstellungen basierend auf den geladenen Daten\n",
    "settings = get_dataset_settings_alldata(data)[dataset_name]\n",
    "sample_size = 50\n",
    "\n",
    "y, train_data, test_data, X_train_features, X_test_features, y_train, y_test, data, dataset_name  = preprocess_data_alldata(\n",
    "    data=data,\n",
    "    dataset_name=dataset_name,\n",
    "    bool_columns=settings['bool_columns'],\n",
    "    drop_columns=settings['drop_columns'],\n",
    "    sample_size=sample_size\n",
    ")\n",
    "\n",
    "display(y_train.head(3))\n",
    "print(f\"Anzahl der Zeilen: {len(y_train)}\")\n",
    "print(\"Anzahl der targets:\", len(y_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDOP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Execution starts here\n",
    "combinations = [(9, 1), (7.5, 2.5), (5, 5), (2.5, 7.5), (1, 9)]\n",
    "table_rows = []\n",
    "random_state = 1\n",
    "global_cv_results = []\n",
    "\n",
    "# Initialize cvFolds\n",
    "cvFolds = None  # Initialization\n",
    "\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "\n",
    "    # Parallelize column processing within each combination with n_jobs=4 to limit threads\n",
    "    column_results = Parallel(n_jobs=1)(\n",
    "        delayed(process_target_alldata)(column, cu, co, tau, y_train, X_train_features, X_test_features, y_test, random_state)\n",
    "        for column in y_train.columns\n",
    "    )\n",
    "\n",
    "    # Combine results from all columns and print after each column\n",
    "    for result in column_results:\n",
    "        table_rows.extend(result)\n",
    "        print(table_rows)\n",
    "        # Convert the latest result to a DataFrame and print it\n",
    "        result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "        print(result_table)  # Print the updated results after each column is processed\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "\n",
    "# Construct the filename using the format \"results_basicModels_{dataset_name}.csv\"\n",
    "filename = f\"FULLDATASET_results_basic_Models_{dataset_name}.csv\"\n",
    "result_table.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Results saved as {filename}\")\n",
    "\n",
    "# Aggregate and save cross-validation results at the end of the entire workflow\n",
    "if global_cv_results:\n",
    "    # Concatenate all cross-validation results into a single DataFrame\n",
    "    aggregated_cv_results_df = pd.concat(global_cv_results, ignore_index=True)\n",
    "\n",
    "    # Print a summary of the aggregated cross-validation data to verify it looks correct\n",
    "    print(\"Aggregated cross-validation results sample:\")\n",
    "    print(aggregated_cv_results_df.head(5))  # Print the first 5 rows as a sample\n",
    "\n",
    "    # Save the aggregated results to a CSV file\n",
    "    aggregated_cv_filename = f\"FULLDATASET_cv_scores_basic_models_{dataset_name}.csv\"\n",
    "    aggregated_cv_results_df.to_csv(aggregated_cv_filename, index=False)\n",
    "    print(f\"Aggregated cross-validation results saved as {aggregated_cv_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRF+Leveset Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Processing cu, co combination: cu=9, co=1\n",
      "Processing column: dummyID\n",
      "Wage Dataset, no time series split using basic KFold Cross Validation\n",
      "[(array([ 2779,  2780,  2781, ..., 13891, 13892, 13893]), array([   0,    1,    2, ..., 2776, 2777, 2778])), (array([    0,     1,     2, ..., 13891, 13892, 13893]), array([2779, 2780, 2781, ..., 5555, 5556, 5557])), (array([    0,     1,     2, ..., 13891, 13892, 13893]), array([5558, 5559, 5560, ..., 8334, 8335, 8336])), (array([    0,     1,     2, ..., 13891, 13892, 13893]), array([ 8337,  8338,  8339, ..., 11113, 11114, 11115])), (array([    0,     1,     2, ..., 11113, 11114, 11115]), array([11116, 11117, 11118, ..., 13891, 13892, 13893]))]\n",
      "Length of X_train_scaled: 13894\n",
      "Length of X_test_scaled: 3476\n",
      "{'binSize': [20, 100, 400], 'weightsByDistance': [True, False]}\n",
      "Evaluating model: LS_KDEx_LGBM, cu: 9, co: 1\n",
      "OrderedDict([('learning_rate', 0.1), ('max_depth', 5), ('min_data_in_leaf', 100), ('n_estimators', 100), ('num_leaves', 127)])\n",
      "Evaluating model: LS_KDEx_MLP, cu: 9, co: 1\n",
      "    Variable  cu  co        Model  Pinball Loss                                   Best Params   delta C   sl\n",
      "13      18.0   9   1  LS_KDEx_MLP      0.087469  {'binSize': 400, 'weightsByDistance': False}  0.219883  0.9\n",
      "14      19.0   9   1  LS_KDEx_MLP      0.094305  {'binSize': 400, 'weightsByDistance': False}  0.189588  0.9\n",
      "15      20.0   9   1  LS_KDEx_MLP      0.083439  {'binSize': 400, 'weightsByDistance': False}  0.153767  0.9\n",
      "16      21.0   9   1  LS_KDEx_MLP      0.105084  {'binSize': 400, 'weightsByDistance': False}  0.206558  0.9\n",
      "17      22.0   9   1  LS_KDEx_MLP      0.106784  {'binSize': 400, 'weightsByDistance': False}  0.180939  0.9\n",
      "Processing cu, co combination: cu=7.5, co=2.5\n",
      "Processing column: dummyID\n",
      "Wage Dataset, no time series split using basic KFold Cross Validation\n",
      "[(array([ 2779,  2780,  2781, ..., 13891, 13892, 13893]), array([   0,    1,    2, ..., 2776, 2777, 2778])), (array([    0,     1,     2, ..., 13891, 13892, 13893]), array([2779, 2780, 2781, ..., 5555, 5556, 5557])), (array([    0,     1,     2, ..., 13891, 13892, 13893]), array([5558, 5559, 5560, ..., 8334, 8335, 8336])), (array([    0,     1,     2, ..., 13891, 13892, 13893]), array([ 8337,  8338,  8339, ..., 11113, 11114, 11115])), (array([    0,     1,     2, ..., 11113, 11114, 11115]), array([11116, 11117, 11118, ..., 13891, 13892, 13893]))]\n",
      "Length of X_train_scaled: 13894\n",
      "Length of X_test_scaled: 3476\n",
      "{'binSize': [20, 100, 400], 'weightsByDistance': [True, False]}\n",
      "Evaluating model: LS_KDEx_LGBM, cu: 7.5, co: 2.5\n",
      "OrderedDict([('learning_rate', 0.1), ('max_depth', 7), ('min_data_in_leaf', 100), ('n_estimators', 100), ('num_leaves', 31)])\n",
      "Evaluating model: LS_KDEx_MLP, cu: 7.5, co: 2.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 92\u001b[0m\n\u001b[1;32m     88\u001b[0m     mlp_model_params \u001b[38;5;241m=\u001b[39m get_grid(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLevelSetKDEx_groupsplit\u001b[39m\u001b[38;5;124m'\u001b[39m, X_train_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     89\u001b[0m     mlp_model_evaluation \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     90\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLS_KDEx_MLP\u001b[39m\u001b[38;5;124m'\u001b[39m, LevelSetKDEx(estimator\u001b[38;5;241m=\u001b[39mmlp_model, binSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, weightsByDistance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), mlp_model_params)\n\u001b[1;32m     91\u001b[0m     ]\n\u001b[0;32m---> 92\u001b[0m     \u001b[43mevaluate_and_append_models_alldata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_model_evaluation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                       \u001b[49m\u001b[43my_train_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaa_pinball_losses_per_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mco\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_scaled_withID\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# Although the Wage data set is not a timeseries, it works similarly well on group timeseries splits. \u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# To do this, we set shuffle = True once in the train/test split. \u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Resulting in an mixed order of the train/test points, even if we order it by the Index.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly time series Data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/WorkingFolder/scripts/train_and_evaluate_alldata.py:51\u001b[0m, in \u001b[0;36mevaluate_and_append_models_alldata\u001b[0;34m(models, X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_losses_per_id, tau, cu, co, column, table_rows, timeseries, X_test_scaled_withID)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model, param_grid \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, cu: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcu\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, co: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mco\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m     pinball_loss_value, best_params, predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model_alldata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mco\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Create DataFrame for model predictions\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     predictions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_for_CV\u001b[39m\u001b[38;5;124m'\u001b[39m: X_test_scaled_withID[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_for_CV\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m'\u001b[39m: y_test_col\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions\n\u001b[1;32m     61\u001b[0m     })\n",
      "File \u001b[0;32m~/WorkingFolder/scripts/train_and_evaluate_alldata.py:96\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model_alldata\u001b[0;34m(model_name, model, param_grid, X_train_scaled, X_test_scaled, y_train, y_test, tau, cu, co, timeseries, column)\u001b[0m\n\u001b[1;32m     92\u001b[0m model\u001b[38;5;241m.\u001b[39mestimator \u001b[38;5;241m=\u001b[39m point_forecast_model\n\u001b[1;32m     94\u001b[0m CV \u001b[38;5;241m=\u001b[39m QuantileCrossValidation(estimator\u001b[38;5;241m=\u001b[39mmodel, parameterGrid\u001b[38;5;241m=\u001b[39mparam_grid, cvFolds\u001b[38;5;241m=\u001b[39mcvFolds_FULLDATA,\n\u001b[1;32m     95\u001b[0m                              probs\u001b[38;5;241m=\u001b[39m[tau], refitPerProb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)\n\u001b[0;32m---> 96\u001b[0m \u001b[43mCV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m fold_scores_raw \u001b[38;5;241m=\u001b[39m CV\u001b[38;5;241m.\u001b[39mcvResults_raw\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_idx, fold_scores \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fold_scores_raw):\n",
      "File \u001b[0;32m~/WorkingFolder/new_enviroment/lib/python3.8/site-packages/dddex/crossValidation.py:109\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    106\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X)\n\u001b[1;32m    107\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y)\n\u001b[0;32m--> 109\u001b[0m scoresPerFold \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgetFoldScore\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43mparameterGrid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameterGrid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43mcvFold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcvFold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcvFold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvFolds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# scoresPerFold = [getFoldScore(estimator = copy.deepcopy(self.estimator),\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m#                               parameterGrid = self.parameterGrid,\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#                               cvFold = cvFold,\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#                               probs = self.probs,\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m#                               y = y,\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m#                               X = X) for cvFold in self.cvFolds]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcvResults_raw \u001b[38;5;241m=\u001b[39m scoresPerFold\n",
      "File \u001b[0;32m~/WorkingFolder/new_enviroment/lib/python3.8/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/WorkingFolder/new_enviroment/lib/python3.8/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/WorkingFolder/new_enviroment/lib/python3.8/site-packages/dddex/crossValidation.py:215\u001b[0m, in \u001b[0;36mgetFoldScore\u001b[0;34m(estimator, parameterGrid, cvFold, probs, X, y)\u001b[0m\n\u001b[1;32m    210\u001b[0m estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    212\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X \u001b[38;5;241m=\u001b[39m XTrainFold,\n\u001b[1;32m    213\u001b[0m               y \u001b[38;5;241m=\u001b[39m yTrainFold)\n\u001b[0;32m--> 215\u001b[0m quantilesDf \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mXTestFold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m costsDict \u001b[38;5;241m=\u001b[39m {prob: getPinballLoss(decisions \u001b[38;5;241m=\u001b[39m quantilesDf\u001b[38;5;241m.\u001b[39mloc[:, prob],\n\u001b[1;32m    219\u001b[0m                                   yTest \u001b[38;5;241m=\u001b[39m yTestFold,\n\u001b[1;32m    220\u001b[0m                                   prob \u001b[38;5;241m=\u001b[39m prob) \u001b[38;5;28;01mfor\u001b[39;00m prob \u001b[38;5;129;01min\u001b[39;00m probs}\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# We have to capture the special case of costSAA == 0, because then we can't compute the \u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Cost-Ratio using the actual definition.\u001b[39;00m\n",
      "File \u001b[0;32m~/WorkingFolder/new_enviroment/lib/python3.8/site-packages/dddex/baseClasses.py:161\u001b[0m, in \u001b[0;36mBaseWeightsBasedEstimator.predict\u001b[0;34m(self, X, probs, scalingList)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     scalingListSubset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m distributionDataList \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetWeights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mXSubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43moutputType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcumulativeDistribution\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mscalingList\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscalingListSubset\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m    165\u001b[0m quantilesList \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m probsDistribution, valuesDistribution \u001b[38;5;129;01min\u001b[39;00m distributionDataList:\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# A tolerance term of 10^-8 is substracted from prob to account for rounding errors due to numerical precision.\u001b[39;00m\n",
      "File \u001b[0;32m~/WorkingFolder/new_enviroment/lib/python3.8/site-packages/dddex/levelSetKDEx_univariate.py:182\u001b[0m, in \u001b[0;36mLevelSetKDEx.getWeights\u001b[0;34m(self, X, outputType, scalingList)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     weightsDataList \u001b[38;5;241m=\u001b[39m [(np\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(neighbors), \u001b[38;5;28mlen\u001b[39m(neighbors)), np\u001b[38;5;241m.\u001b[39marray(neighbors, dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muintc\u001b[39m\u001b[38;5;124m'\u001b[39m)) \n\u001b[1;32m    180\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m neighbors \u001b[38;5;129;01min\u001b[39;00m neighborsList]\n\u001b[0;32m--> 182\u001b[0m     weightsDataList \u001b[38;5;241m=\u001b[39m \u001b[43mrestructureWeightsDataList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweightsDataList\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweightsDataList\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43moutputType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutputType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myTrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mscalingList\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscalingList\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mequalWeights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weightsDataList\n",
      "File \u001b[0;32m~/WorkingFolder/new_enviroment/lib/python3.8/site-packages/dddex/utils.py:97\u001b[0m, in \u001b[0;36mrestructureWeightsDataList\u001b[0;34m(weightsDataList, outputType, y, scalingList, equalWeights)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(weightsDataList)):\n\u001b[1;32m     95\u001b[0m     weightsPos, yWeightPos \u001b[38;5;241m=\u001b[39m weightsDataList[i][\u001b[38;5;241m0\u001b[39m], y[weightsDataList[i][\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m---> 97\u001b[0m     indicesSort \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43myWeightPos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     weightsPosSorted \u001b[38;5;241m=\u001b[39m weightsPos[indicesSort]\n\u001b[1;32m    100\u001b[0m     yWeightPosSorted \u001b[38;5;241m=\u001b[39m yWeightPos[indicesSort]\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/WorkingFolder/new_enviroment/lib/python3.8/site-packages/numpy/core/fromnumeric.py:1038\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_argsort_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a,)\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margsort\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \n\u001b[1;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margsort\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, kind\u001b[38;5;241m=\u001b[39mkind, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timeseries = True\n",
    "import scripts.config as config\n",
    "from scripts.config import *\n",
    "importlib.reload(config)\n",
    "\n",
    "print(n_jobs)\n",
    "\n",
    "# Execution starts here\n",
    "combinations = [(9, 1), (7.5, 2.5), (5, 5), (2.5, 7.5), (1, 9)]\n",
    "\n",
    "table_rows = []\n",
    "random_state = 1\n",
    "drf_cv_results = []\n",
    "global_fold_scores = []\n",
    "\n",
    "# Iterate over combinations and process them directly\n",
    "for cu, co in combinations:\n",
    "    print(f\"Processing cu, co combination: cu={cu}, co={co}\")\n",
    "    tau = cu / (cu + co)\n",
    "    with threadpool_limits(limits=1):\n",
    "            for column in y_train.columns:\n",
    "                print(f\"Processing column: {column}\")\n",
    "\n",
    "                              # Preprocess data\n",
    "                X_train_scaled, X_test_scaled, y_train_col, y_test_col, X_train_scaled_withID, X_test_scaled_withID = preprocess_per_instance_alldata(\n",
    "                    column, X_train_features, X_test_features, y_train, y_test\n",
    "                )\n",
    "\n",
    "                create_cv_folds_alldata(X_train_scaled_withID)\n",
    "                \n",
    "                # SAA model\n",
    "                saa_model = SampleAverageApproximationNewsvendor(cu, co)\n",
    "                saa_pred = saa_model.fit(y_train_col).predict(X_test_scaled.shape[0])\n",
    "\n",
    "\n",
    "                # Ensure id_for_CV, y_true, and y_pred are 1-D arrays\n",
    "                id_for_CV = X_test_scaled_withID['id_for_CV'].values.flatten()\n",
    "                y_true = y_test_col.values.flatten()\n",
    "                y_pred = saa_pred.flatten()  # Flatten y_pred to ensure it's 1-D\n",
    "\n",
    "                # Create DataFrame for SAA predictions\n",
    "                saa_predictions_df = pd.DataFrame({\n",
    "                    'id_for_CV': id_for_CV,\n",
    "                    'y_true': y_true,\n",
    "                    'y_pred': y_pred  # Use the flattened y_pred here\n",
    "                })\n",
    "\n",
    "                saa_pinball_losses_per_id = {}\n",
    "                grouped_saa = saa_predictions_df.groupby('id_for_CV')\n",
    "                for id_val, group in grouped_saa:\n",
    "                    y_true_id = group['y_true'].values\n",
    "                    y_pred_id = group['y_pred'].values\n",
    "                    pinball_loss_id = pinball_loss(y_true_id, y_pred_id, tau)\n",
    "                    saa_pinball_losses_per_id[id_val] = pinball_loss_id\n",
    "                    append_result(table_rows, id_val, cu, co, 'SAA', pinball_loss_id, 'N/A', np.nan, tau)\n",
    "\n",
    "                n_features = X_train_scaled.shape[1]\n",
    "\n",
    "                drf_model = DRFWrapper(min_node_size=10, num_trees=100, num_threads=2)\n",
    "                drf_grid = get_grid('DRF', X_train_scaled.shape[1])\n",
    "                \n",
    "                print(f\"Length of X_train_scaled: {len(X_train_scaled)}\")\n",
    "                print(f\"Length of X_test_scaled: {len(X_test_scaled)}\")\n",
    "                #evaluate_and_append_models([('DRF', drf_model, drf_grid)], X_train_scaled, X_test_scaled, y_train_col, y_test_col, saa_pinball_losses_per_id, tau, cu, co, column, table_rows, timeseries, X_test_scaled_withID)\n",
    "\n",
    "\n",
    "            \n",
    "                if timeseries:\n",
    "                    # Initialisiere LGBM und MLP-Modelle\n",
    "                    lgbm_model = LGBMRegressor(random_state=random_state, n_jobs=n_jobs, verbosity=-1)\n",
    "                    mlp_model = MLPRegressorWrapper(random_state=random_state, early_stopping=True)\n",
    "\n",
    "                    # LGBM-Modell mit GroupSplitting evaluieren\n",
    "                    lgbm_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    print(lgbm_model_params)\n",
    "\n",
    "                    lgbm_model_evaluation = [\n",
    "                        ('LS_KDEx_LGBM', LevelSetKDEx(estimator=lgbm_model, binSize=100, weightsByDistance=False), lgbm_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models_alldata(lgbm_model_evaluation, X_train_scaled, X_test_scaled,\n",
    "                                       y_train_col, y_test_col, saa_pinball_losses_per_id,\n",
    "                                       tau, cu, co, column, table_rows, timeseries, X_test_scaled_withID)\n",
    "\n",
    "                    # MLP-Modell mit GroupSplitting evaluieren\n",
    "                    mlp_model_params = get_grid('LevelSetKDEx_groupsplit', X_train_scaled.shape[1])\n",
    "                    mlp_model_evaluation = [\n",
    "                        ('LS_KDEx_MLP', LevelSetKDEx(estimator=mlp_model, binSize=100, weightsByDistance=False), mlp_model_params)\n",
    "                    ]\n",
    "                    evaluate_and_append_models_alldata(mlp_model_evaluation, X_train_scaled, X_test_scaled,\n",
    "                                       y_train_col, y_test_col, saa_pinball_losses_per_id,\n",
    "                                       tau, cu, co, column, table_rows, timeseries, X_test_scaled_withID)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Although the Wage data set is not a timeseries, it works similarly well on group timeseries splits. \n",
    "                    # To do this, we set shuffle = True once in the train/test split. \n",
    "                    # Resulting in an mixed order of the train/test points, even if we order it by the Index.\n",
    "                    print(\"Only time series Data\")\n",
    "\n",
    "                # Print the table after evaluating each column\n",
    "                second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "                print(second_result_table.tail(5))  # Print the last 5 rows of the table after each column is processed\n",
    "\n",
    "\n",
    "# Final result table after processing all combinations\n",
    "second_result_table = pd.DataFrame(table_rows, columns=['Variable', 'cu', 'co', 'Model', 'Pinball Loss', 'Best Params', 'delta C', 'sl'])\n",
    "filename = f\"results_LevelsetModels_{dataset_name}.csv\"\n",
    "\n",
    "\n",
    "second_result_table.to_csv(filename, index=False)\n",
    "# Define the folder where results will be saved\n",
    "results_folder = \"results\"\n",
    "\n",
    "from scripts.globals import global_fold_scores, global_cv_results, drf_cv_results\n",
    "\n",
    "\n",
    "# Construct the filename and save it in the \"results\" folder\n",
    "filename = os.path.join(results_folder, f\"results_LevelsetModels_{dataset_name}.csv\")\n",
    "second_result_table.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Results saved as {filename}\")\n",
    "\n",
    "# Aggregating fold-wise cross-validation results\n",
    "if global_fold_scores:  # No 'globals' function, just directly use 'global_fold_scores'\n",
    "    # Reset multi-index for all fold scores\n",
    "    global_fold_scores_flat = []\n",
    "    for fold_scores_df in global_fold_scores:\n",
    "        # Reset the multi-index so that the binSize and weightsByDistance become normal columns\n",
    "        flat_df = fold_scores_df.reset_index()\n",
    "        global_fold_scores_flat.append(flat_df)\n",
    "\n",
    "    # Concatenate all fold-wise cross-validation results into a single DataFrame\n",
    "    aggregated_fold_scores_df = pd.concat(global_fold_scores_flat, ignore_index=True)\n",
    "\n",
    "    # Save the aggregated results to a CSV file in the \"results\" folder\n",
    "    aggregated_fold_scores_filename = os.path.join(results_folder, f\"cv_scores_levelset_models_{dataset_name}.csv\")\n",
    "    aggregated_fold_scores_df.to_csv(aggregated_fold_scores_filename, index=False)\n",
    "\n",
    "    ### DRF DATA INSERTED INTO THE MAIN TABLE WHERE OTHER BAYES CVs ARE STORED\n",
    "    aggregated_drf_cv_results_df = pd.concat(drf_cv_results, ignore_index=True)\n",
    "\n",
    "    # Save the aggregated DRF results to a CSV file in the \"results\" folder\n",
    "    aggregated_cv_filename = os.path.join(results_folder, f\"cv_drf_scores_{dataset_name}.csv\")\n",
    "    aggregated_drf_cv_results_df.to_csv(aggregated_cv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter die Tabelle, um nur die Zeilen mit dem Modell \"SAA\" anzuzeigen\n",
    "saa_results = result_table[result_table['Model'] == 'SAA']\n",
    "\n",
    "# Zeige die ersten 20 Zeilen der gefilterten Tabelle an\n",
    "print(saa_results.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_enviroment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
